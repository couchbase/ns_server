%% @author Couchbase <info@couchbase.com>
%% @copyright 2009-Present Couchbase, Inc.
%%
%% Use of this software is governed by the Business Source License included in
%% the file licenses/BSL-Couchbase.txt.  As of the Change Date specified in that
%% file, in accordance with the Business Source License, use of this software
%% will be governed by the Apache License, Version 2.0, included in the file
%% licenses/APL2.txt.
%%
-module(ns_cluster).

-behaviour(gen_server).

-include("ns_common.hrl").
-include_lib("ns_common/include/cut.hrl").

-ifdef(TEST).
-include_lib("eunit/include/eunit.hrl").
-endif.

-define(UNUSED_NODE_JOIN_REQUEST, 2).
-define(NODE_JOINED, 3).
-define(NODE_EJECTED, 4).
-define(NODE_JOIN_FAILED, 5).

-define(ADD_NODE_TIMEOUT,       ?get_timeout(add_node, 240000)).
-define(ENGAGE_TIMEOUT,         ?get_timeout(engage, 30000)).
-define(COMPLETE_TIMEOUT,       ?get_timeout(complete, 240000)).
-define(CHANGE_ADDRESS_TIMEOUT, ?get_timeout(change_address, 30000)).
-define(HARD_RESET_TIMEOUT,     ?get_timeout(hard_reset, 240000)).

-define(cluster_log(Code, Fmt, Args),
        ale:xlog(?USER_LOGGER, ns_log_sink:get_loglevel(?MODULE, Code),
                 {?MODULE, Code}, Fmt, Args)).

-define(cluster_debug(Fmt, Args), ale:debug(?CLUSTER_LOGGER, Fmt, Args)).
-define(cluster_info(Fmt, Args), ale:info(?CLUSTER_LOGGER, Fmt, Args)).
-define(cluster_warning(Fmt, Args), ale:warn(?CLUSTER_LOGGER, Fmt, Args)).
-define(cluster_error(Fmt, Args), ale:error(?CLUSTER_LOGGER, Fmt, Args)).

%% gen_server callbacks
-export([code_change/3, handle_call/3, handle_cast/2, handle_info/2, init/1,
         terminate/2]).

%% API
-export([leave/0,
         leave/1,
         hard_reset_init/0,
         hard_reset/0,
         shun/1,
         start_link/0]).

-export([add_node_to_group/6,
         engage_cluster/1, complete_join/1,
         add_node_timeout/0,
         check_host_port_connectivity/2, change_address/1,
         enforce_topology_limitation/1,
         rename_marker_path/0,
         sanitize_node_info/1,
         verify_otp_connectivity/2,
         is_host_allowed/1,
         is_host_allowed/2,
         allowed_hosts/0]).

%% debugging & diagnostic export
-export([do_change_address/2]).

-export([counters/0,
         counter/3,
         counter_inc/1,
         counter_inc/2]).

-record(state, {}).

%%
%% API
%%

start_link() ->
    gen_server:start_link({local, ?MODULE}, ?MODULE, [], []).

add_node_to_group(Scheme, RemoteAddr, RestPort, HiddenAuth, GroupUUID, Services) ->
    RV = gen_server:call(?MODULE,
                         {add_node_to_group, Scheme, RemoteAddr, RestPort,
                          HiddenAuth, GroupUUID, Services},
                         ?ADD_NODE_TIMEOUT),
    case RV of
        {error, _What, Message} ->
            ?cluster_log(?NODE_JOIN_FAILED,
                         "Failed to add node ~s:~w to cluster. ~s",
                         [RemoteAddr, RestPort, Message]);
        _ -> ok
    end,
    RV.

engage_cluster(NodeKVList) ->
    ?cluster_debug("Processing engage cluster request with ~p.",
                   [sanitize_node_info(NodeKVList)]),
    MyNode = node(),
    RawOtpNode = proplists:get_value(<<"otpNode">>, NodeKVList, <<"undefined">>),
    OtpNode = binary_to_atom(RawOtpNode, latin1),
    Joinable = ns_cluster_membership:system_joinable(),
    if
        OtpNode =:= MyNode ->
            {error, self_join, <<"Joining node to itself is not allowed.">>};
        not Joinable ->
            {error, system_not_joinable,
             <<"Node is already part of cluster.">>};
        true ->
            engage_cluster_not_to_self(NodeKVList)
    end.

engage_cluster_not_to_self(NodeKVList) ->
    case cluster_compat_mode:tls_supported() of
        true ->
            engage_cluster_apply_certs(NodeKVList);
        false ->
            engage_cluster_apply_net_config(NodeKVList)
    end.

engage_cluster_apply_certs(NodeKVList) ->
    CanRewriteNodeCerts =
        ns_server_cert:this_node_uses_self_generated_certs(),
    CanRewriteClientCerts =
        ns_server_cert:this_node_uses_self_generated_client_certs(),

    %% 7.0+ nodes wrap keys in {otpCookie, } to make sure it
    %% is sanitized by pre 7.0 nodes
    UnwrapValue = fun ({[{<<"otpCookie">>, K}]}) -> K;
                      (K) -> K
                  end,

    %% 6.5 - Added autogeneratedCA
    %% 7.0 - Added autogeneratedCert
    %% 7.6 - Added autogeneratedClientCert
    case proplists:get_value(<<"autogeneratedCA">>, NodeKVList) of
        undefined ->
            %% We assume the remote node uses custom certs in this case,
            %% so try to load custom certs from inbox as well. Strictly
            %% speaking, by this moment certs should have been loaded
            %% already (otherwise a tls connection for /engageCluster call
            %% won't be possible), but there are some cases when it's still
            %% possible that user hasn't loaded certs yet. For example,
            %% when the remote node is pre-7.1, it might use http
            %% for /engageCluster.
            case apply_certs() of
                ok -> engage_cluster_apply_net_config(NodeKVList);
                {error, _, _} = Error -> Error
            end;
        Cert when Cert =/= undefined, CanRewriteNodeCerts andalso
                                      CanRewriteClientCerts ->
            NodeCert = proplists:get_value(<<"autogeneratedCert">>,
                                           NodeKVList),
            NodeKey = UnwrapValue(proplists:get_value(<<"autogeneratedKey">>,
                                                         NodeKVList)),
            HostBin = proplists:get_value(
                        <<"requestedTargetNodeHostname">>, NodeKVList),
            Host = binary_to_list(HostBin),
            ClientCert = proplists:get_value(<<"autogeneratedClientCert">>,
                                             NodeKVList),
            ClientKey = UnwrapValue(
                          proplists:get_value(<<"autogeneratedClientKey">>,
                                              NodeKVList)),
            ns_ssl_services_setup:set_certs(Host, Cert, NodeCert,
                                            NodeKey, ClientCert, ClientKey),
            ns_ssl_services_setup:sync(),
            ?log_info("Generated certificate was loaded on the node "
                      "before joining. Cert: ~p", [Cert]),
            engage_cluster_apply_net_config(NodeKVList);
        _ ->
            ?log_info("Not using autogenerated certs because this node uses "
                      "custom certs"),
            engage_cluster_apply_net_config(NodeKVList)
    end.

apply_certs() ->
    ReadCAsRes = case ns_server_cert:load_CAs_from_inbox() of
                    {ok, _} -> ok;
                    {error, {_Dir, empty}} -> ok;
                    {error, {_Dir, {read, enoent}}} -> ok;
                    {error, _} = E -> E
                end,
    case ReadCAsRes of
        ok ->
            %% We skip loading if certs in inbox are already loaded because of
            %% two reasons:
            %%  - We don't have passphrase settings for encrypted key, so if we
            %%    try to load them one more time, we will fail (in case if pkey
            %%    is encrypted);
            %%  - No point in reloading the same certs twice anyway
            %% Note 1:
            %% The check is implemented outside of load_certs_from_inbox
            %% because it actually does make sense calling
            %% load_certs_from_inbox with unchanged cert files,
            %% when only passphrase settings change.
            %% Note 2:
            %% This loading of certs from engage_cluster is only useful
            %% when node is loaded via http (not https). In case of https
            %% certificates must already be loaded by this moment, we won't be
            %% able to establish a TLS connection for engage_cluster otherwise.
            %% Note 3:
            %% We only try loading non-pkcs12 certs here mostly because of the
            %% following reasons:
            %%  * This code is for backward compatibility only, and we don't
            %%    have to be backward compatible when PKCS12 file is used.
            %%    P12 file should always be loaded explicitly before the join;
            %%  * We can't tell for sure if we already loaded
            %%    certs from this file or not without a password (because in p12
            %%    file cert chain is also encrypted).
            InboxChainFile = ns_server_cert:inbox_chain_path(node_cert),
            ShouldTryLoadCerts =
                filelib:is_file(InboxChainFile) andalso
                (not ns_server_cert:is_cert_loaded_from_file(InboxChainFile)),
            case ShouldTryLoadCerts andalso
                 ns_server_cert:load_certs_from_inbox(node_cert, []) of
                false ->
                    ?log_info("Certificates are already loaded or not "
                              "provided, skipping apply_certs stage"),
                    ok;
                {ok, Props, WarningList} ->
                    ns_ssl_services_setup:sync(),
                    ?log_info("Custom certificate was loaded on the node "
                              "before joining. Props: ~p, WarningList: ~p",
                              [Props, WarningList]),
                    ok;
                {error, Error} ->
                    Message =
                        iolist_to_binary(
                          ["Error applying node certificate. ",
                           ns_error_messages:reload_node_certificate_error(
                             Error)]),
                    {error, apply_cert, Message}
            end;
        {error, Error} ->
            Msg = ns_error_messages:load_CAs_from_inbox_error(Error),
            {error, apply_cert, iolist_to_binary(Msg)}
    end.

engage_cluster_apply_net_config(NodeKVList) ->
    case apply_net_config(NodeKVList) of
        ok -> call_engage_cluster(NodeKVList);
        {error, Msg} ->
            {error, apply_net_config, Msg}
    end.

apply_net_config(NodeKVList) ->
    case ensure_dist_ports_match(NodeKVList) of
        ok ->
            {AFamily, AFamilyOnly, NEncryption, Protos, ClientCert} =
                extract_remote_cluster_net_settings(NodeKVList),
            ?log_info("Applying net config. AFamily: ~p, "
                      "AFamilyOnly: ~p, "
                      "NEncryption: ~p, "
                      "DistProtos: ~p"
                      "ClientCert: ~p",
                      [AFamily, AFamilyOnly, NEncryption, Protos, ClientCert]),
            Props = [{externalListeners, Protos},
                     {afamily, AFamily},
                     {afamilyOnly, AFamilyOnly},
                     {nodeEncryption, NEncryption},
                     {clientCertVerification, ClientCert}],
            case netconfig_updater:apply_config(Props) of
                ok ->
                    ns_config:sync_announcements(),
                    menelaus_event:sync(chronicle_compat_events:event_manager()),
                    cluster_compat_mode:is_enterprise() andalso
                        ns_ssl_services_setup:sync(),
                    ok;
                {error, {node_resolution_failed,
                         {AddrFamily, Hostname, Reason}} = ErrorTerm} ->
                    M1 = netconfig_updater:format_error(ErrorTerm),
                    M2 = ns_error_messages:engage_cluster_error(
                           {engage_cluster_failed, {"cluster", Reason, M1,
                                                    Hostname, {AddrFamily}}}),
                    {error, M2};
                {error, ErrorTerm} ->
                    Msg = iolist_to_binary(
                            netconfig_updater:format_error(ErrorTerm)),
                    {error, Msg}
            end;
        {error, Msg} -> {error, Msg}
    end.

extract_remote_cluster_net_settings(NodeKVList) ->
    Listeners = case proplists:get_value(<<"externalListeners">>, NodeKVList) of
                    undefined -> undefined;
                    Pairs ->
                       lists:map(
                         fun ({P}) ->
                                 [{<<"afamily">>, AF},
                                  {<<"nodeEncryption">>, NE}] = lists:usort(P),
                                 {binary_to_atom(AF, latin1), NE}
                         end, Pairs)
                end,
    NEncryption = proplists:get_value(<<"nodeEncryption">>, NodeKVList,
                                      false),
    AFamilyOnly = proplists:get_value(<<"addressFamilyOnly">>, NodeKVList,
                                      false),
    AFamilyBin = proplists:get_value(<<"addressFamily">>, NodeKVList),
    AFamily = binary_to_atom(AFamilyBin, latin1),
    N2NClientCert = proplists:get_value(
                      <<"nodeEncryptionClientCertVerification">>,
                      NodeKVList, false),
    {AFamily, AFamilyOnly, NEncryption, Listeners, N2NClientCert}.

ensure_dist_ports_match(NodeKVList) ->
    {Ports} = proplists:get_value(<<"ports">>, NodeKVList,
                                  {[]}),
    RemoteNode = expect_json_property_atom(<<"otpNode">>, NodeKVList),
    RemoteShortNode = misc:node_name_short(RemoteNode),
    LocalTCPPort = cb_epmd:port_for_node(inet_tcp_dist, RemoteShortNode),
    RemoteTCPPort = proplists:get_value(<<"distTCP">>, Ports, LocalTCPPort),
    LocalTLSPort = cb_epmd:port_for_node(inet_tls_dist, RemoteShortNode),
    RemoteTLSPort = proplists:get_value(<<"distTLS">>, Ports, LocalTLSPort),
    case {RemoteTCPPort, RemoteTLSPort} of
        {LocalTCPPort, LocalTLSPort} -> ok;
        {LocalTCPPort, _} ->
            Msg = io_lib:format("Distribution TLS ports doesn't match. Cluster "
                                "uses ~p while new node will use ~p port",
                                [RemoteTLSPort, LocalTLSPort]),
            {error, iolist_to_binary(Msg)};
        {_, _} ->
            Msg = io_lib:format("Distribution ports doesn't match. Cluster "
                                "uses ~p while new node will use ~p port",
                                [RemoteTCPPort, LocalTCPPort]),
            {error, iolist_to_binary(Msg)}
    end.

call_engage_cluster(NodeKVList) ->
    NodeKVListThunk  = fun () -> NodeKVList end,
    gen_server:call(?MODULE, {engage_cluster, NodeKVListThunk}, ?ENGAGE_TIMEOUT).

complete_join(NodeKVList) ->
    NodeKVListThunk  = fun () -> NodeKVList end,
    gen_server:call(?MODULE, {complete_join, NodeKVListThunk},
                    ?COMPLETE_TIMEOUT).

add_node_timeout() ->
    ?ADD_NODE_TIMEOUT.

-spec change_address(string()) -> ok
                                  | {cannot_resolve, {inet:posix(), inet|inet6}}
                                  | {cannot_listen, inet:posix()}
                                  | not_self_started
                                  | {address_save_failed, any()}
                                  | {address_not_allowed, string()}
                                  | already_part_of_cluster
                                  | not_renamed.
change_address(Address) ->
    case misc:is_good_address(Address) of
        ok ->
            gen_server:call(?MODULE, {change_address, Address}, ?CHANGE_ADDRESS_TIMEOUT);
        Error ->
            Error
    end.

%% @doc Returns proplist of cluster-wide counters.
counters() -> counters(direct).

counters(Snapshot) ->
    Counters = chronicle_compat:get(Snapshot, counters, #{default => []}),
    lists:map(
      fun ({Name, {_Timestamp, Value}}) -> {Name, Value};
          %% backward compat for counters that we kept in ns_config:
          (Pair) -> Pair
      end, Counters).

counter(Snapshot, CounterName, Default) ->
    proplists:get_value(CounterName, counters(Snapshot), Default).

%% @doc Increment a cluster-wide counter.
counter_inc(CounterName) ->
    % We expect counters to be slow moving (num rebalances, num failovers,
    % etc), and favor efficient reads over efficient writes.
    chronicle_kv:transaction(
      kv,
      [counters],
      fun (Snapshot) ->
              {Counters, _} = maps:get(counters, Snapshot, {[], undefined}),
              {_, OldValue} = proplists:get_value(CounterName, Counters,
                                                  {undefined, 0}),
              TS = os:system_time(second),
              NewCounters = [{CounterName, {TS, OldValue + 1}} |
                             proplists:delete(CounterName, Counters)],
              {commit, [{set, counters, NewCounters}]}
      end).

counter_inc(Type, Name)
  when is_atom(type), is_atom(Name) ->
    counter_inc(list_to_atom(atom_to_list(Type) ++ "_" ++ atom_to_list(Name))).

%%
%% gen_server handlers
%%

code_change(_OldVsn, State, _Extra) ->
    {ok, State}.

sanitize_node_info(NodeKVList) ->
    misc:rewrite_tuples(
      fun ({<<"otpCookie">>, Cookie}) ->
              {stop, {<<"otpCookie">>, ns_cookie_manager:sanitize_cookie(Cookie)}};
          ({otpCookie, Cookie}) ->
              {stop, {otpCookie, ns_cookie_manager:sanitize_cookie(Cookie)}};
          ({<<"autogeneratedKey">>, _}) ->
              {stop, {<<"autogeneratedKey">>, <<"********">>}};
          ({<<"autogeneratedClientKey">>, _}) ->
              {stop, {<<"autogeneratedClientKey">>, <<"********">>}};
          (_Other) ->
              continue
      end, NodeKVList).

handle_call({add_node_to_group, Scheme, RemoteAddr, RestPort, HiddenAuth,
             GroupUUID, Services}, _From, State) ->
    ?cluster_debug("handling add_node(~p, ~p, ~p, ~p, ..)",
                   [Scheme, RemoteAddr, RestPort, GroupUUID]),
    RV = do_add_node(Scheme, RemoteAddr, RestPort, HiddenAuth, GroupUUID, Services),
    ?cluster_debug("add_node(~p, ~p, ~p, ~p, ..) -> ~p",
                   [Scheme, RemoteAddr, RestPort, GroupUUID, RV]),
    {reply, RV, State};

handle_call({engage_cluster, NodeKVListThunk}, _From, State) ->
    NodeKVList = NodeKVListThunk(),
    ?cluster_debug("handling engage_cluster(~p)", [sanitize_node_info(NodeKVList)]),
    RV = do_engage_cluster(NodeKVList),
    ?cluster_debug("engage_cluster(..) -> ~p", [RV]),
    {reply, RV, State};

handle_call({complete_join, NodeKVListThunk}, _From, State) ->
    NodeKVList = NodeKVListThunk(),
    ?cluster_debug("handling complete_join(~p)", [sanitize_node_info(NodeKVList)]),
    RV = do_complete_join(NodeKVList),
    ?cluster_debug("complete_join(~p) -> ~p", [sanitize_node_info(NodeKVList), RV]),

    case RV of
        %% we failed to start ns_server back; we don't want to stay in this
        %% state, so perform_actual_join has created a marker file indicating
        %% that somebody needs to attempt to start ns_server back; usually
        %% this somebody is ns_cluster:init; so to let it do its job we need
        %% to restart ns_cluster; note that we still reply to the caller
        {error, {actual_join_failed, Reason}, _} ->
            {stop, Reason, RV, State};
        _ ->
            {reply, RV, State}
    end;

handle_call({change_address, Address}, _From, State) ->
    ?cluster_info("Changing address to ~p due to client request", [Address]),
    RV = case ns_cluster_membership:system_joinable() of
             true ->
                 %% we're the only node in the cluster; allowing rename
                 do_change_address(Address, true);
             false ->
                 already_part_of_cluster
         end,
    {reply, RV, State};
handle_call(hard_reset_init, _From, State) ->
    leave_init(),
    {reply, ok, State};
handle_call(hard_reset, _From, State) ->
    leave_body(),
    {reply, ok, State}.

leave_init() ->
    misc:create_marker(leave_marker_path()).

leave_body() ->
    ?cluster_log(0001, "Node ~p is leaving cluster.", [node()]),

    %% stop nearly everything
    ok = ns_server_cluster_sup:stop_ns_server(),

    perform_leave(),

    ?cluster_debug("Leaving cluster", []),

    misc:create_marker(start_marker_path()),
    misc:remove_marker(leave_marker_path()),

    ok = ns_server_cluster_sup:start_ns_server(),
    ns_ports_setup:restart_memcached(),

    misc:remove_marker(start_marker_path()).

handle_cast(leave, State) ->
    leave_body(),
    {noreply, State};
handle_cast(repair_join, State) ->
    ?log_debug("Repair after unsuccessful join."),
    perform_leave(),

    misc:remove_marker(join_marker_path()),
    {noreply, State};

handle_cast(retry_start_ns_server, State) ->
    case ns_server_cluster_sup:start_ns_server() of
        ok ->
            ok;
        {error, running} ->
            ?log_warning("ns_server is already running. Ignoring."),
            ok;
        Other ->
            exit({failed_to_start_ns_server, Other})
    end,

    %% We have unfinished cluster join or cluster leave.
    %% We should always restart memcached in those cases, so that
    %% it can pick up new credentials.
    case misc:marker_exists(start_marker_path()) of
        true ->
            ?log_info("Found marker ~p. "
                      "Looks like we failed to restart ns_server and memcached"
                      "after leaving or joining a cluster. "
                      "Ns_server is running now, but memcached still needs to "
                      "be restarted. "
                      "Will restart memcached now.", [start_marker_path()]),
            ns_ports_setup:restart_memcached(),
            misc:remove_marker(start_marker_path());
        false ->
            ok
    end,

    {noreply, State}.

handle_info(check_chronicle_state, State) ->
    ChronicleState = chronicle:get_system_state(),
    ?log_info("Chronicle state is: ~p", [ChronicleState]),
    case ChronicleState of
        removed ->
            false = misc:marker_exists(leave_marker_path()),
            leave_init(),
            leave_body();
        _ ->
            ok
    end,
    {noreply, State};

handle_info(Msg, State) ->
    ?cluster_debug("Unexpected message ~p, State = ~p", [Msg, State]),
    {noreply, State}.


init([]) ->
    Self = self(),

    Subscribe =
        fun () ->
                ns_pubsub:subscribe_link(
                  chronicle_external_events,
                  fun ({important_change, system_state}) ->
                          Self ! check_chronicle_state;
                      (_) ->
                          ok
                  end)
        end,

    case misc:marker_exists(leave_marker_path()) of
        true ->
            ?log_info("Found marker of in-flight cluster leave. "
                      "Looks like previous leave procedure crashed. "
                      "Going to complete leave cluster procedure."),
            %% we have to do it async because otherwise our parent
            %% supervisor is waiting us to complete init and our call
            %% to terminate ns_server_sup is going to cause deadlock
            gen_server:cast(Self, leave),
            Subscribe();
        false ->
            IsNodeRemoved = chronicle:get_system_state() =:= removed,
            case misc:marker_exists(join_marker_path()) of
                true ->
                    ?log_info("Found marker ~p. "
                                "Looks like we failed in process of "
                                "joining cluster. Will restore config "
                                "to clean state. ", [join_marker_path()]),
                    gen_server:cast(Self, repair_join);
                false when IsNodeRemoved ->
                    ?log_info("Node is removed from cluster. "),
                    %% This can happen if ns_server start fails during
                    %% join process. The other node (that sends
                    %% /completeJoin) will remove this
                    %% node from cluster as soon as it receives the
                    %% completeJoin response (error).
                    %% It is possible that if we don't repair_join here
                    %% ns_cluster will try starting ns_server
                    %% again and again until it succeeds (only after
                    %% that it will leave the cluster). The problem is
                    %% that it may be unable to start ns_server
                    %% (because chronicle is dead already), so it will
                    %% be stuck in the loop forever.
                    gen_server:cast(Self, repair_join);
                false ->
                    ok
            end,
            gen_server:cast(Self, retry_start_ns_server),
            Subscribe(),
            Self ! check_chronicle_state
    end,
    {ok, #state{}}.


terminate(_Reason, _State) ->
    ok.


%%
%% Internal functions
%%


%%
%% API
%%

leave() ->
    RemoteNode = ns_node_disco:another_live_node(),
    ?cluster_log(?NODE_EJECTED, "Node ~s asked to leave the cluster", [node()]),
    %% MB-3160: sync any pending config before we leave, to make sure,
    %% say, deactivation of membership isn't lost
    ok = ns_config_rep:ensure_config_seen_by_nodes([RemoteNode]),
    ?cluster_debug("ns_cluster: leaving the cluster from ~p.",
                   [RemoteNode]),
    %% Tell the remote server to tell everyone to shun me.
    rpc:cast(RemoteNode, ?MODULE, shun, [node()]).

%% Cause another node to leave the cluster if it's up
leave(Node) ->
    ?cluster_debug("Asking node ~p to leave the cluster", [Node]),

    case Node =:= node() of
        true ->
            leave();
        false ->
            shun(Node)
    end.

%% Just leave the cluster without waiting for a removal notification from a
%% chronicle peer. In effect, this is a hard reset of the node. This is intended
%% for use after an unsafe failover situation. We don't get a notification of
%% chronicle state change to removed after an unsafe failover - which would have
%% called leave_body().
hard_reset() ->
    ?cluster_log(0001, "Hard reset of node ~p in progress.", [node()]),
    gen_server:call(?MODULE, hard_reset, ?HARD_RESET_TIMEOUT).

hard_reset_init() ->
    ?cluster_log(0001, "Hard reset of node ~p initiated.", [node()]),
    gen_server:call(?MODULE, hard_reset_init).

%% Note that shun does *not* cause the other node to reset its config!
shun(RemoteNode) ->
    case RemoteNode =:= node() of
        false ->
            try
                ?cluster_debug("Shunning ~p", [RemoteNode]),
                case chronicle_master:remove_peer(RemoteNode) of
                    ok ->
                        ns_config_rep:ensure_config_pushed();
                    delegated_operation ->
                        ok
                end
            catch T:E:Stack ->
                    ?log_error("Shun failed with ~p", [{T,E,Stack}]),
                    exit(shun_failed)
            end;
        true ->
            ?cluster_debug("Asked to shun myself. Leaving cluster.", []),
            leave()
    end.

check_host_port_connectivity(Host, Port, AFamily) ->
    try gen_tcp:connect(Host, Port, [AFamily, binary, {packet, 0},
                                     {active, false}], 5000) of
        {ok, Socket} ->
            try
                {ok, {IpAddr, _}} = inet:sockname(Socket),
                ?log_debug("Successfully checked TCP connectivity to "
                           "~p:~p", [Host, Port]),
                {ok, inet:ntoa(IpAddr)}
            after
                gen_tcp:close(Socket)
            end;
        {error, nxdomain} ->
            {error, {AFamily, nxdomain}};
        {error, Reason} ->
            {error, Reason}
    catch
        _:R -> {error, R}
    end.

resolve(Host) ->
    Res = [{inet:getaddr(Host, AF), AF} || AF <- [inet, inet6]],
    IPs = [{IP, AF} || {{ok, IP}, AF} <- Res],
    Errors = [R || {{error, R}, _} <- Res],
    case IPs of
        [] -> {error, Errors};
        _ -> {ok, IPs}
    end.

check_host_port_connectivity(Host, Port) ->
    case resolve(Host) of
        {ok, IPList} ->
            lists:foldl(
              fun ({IP, AFamily}, {error, _}) ->
                      case check_host_port_connectivity(IP, Port, AFamily) of
                          {ok, MyIP} -> {ok, MyIP, AFamily};
                          {error, Error} -> {error, Error}
                      end;
                  (_, Res) ->
                      Res
              end, {error, undefined}, IPList);
        {error, Errors} ->
            {error, hd(Errors)}
    end.

-spec do_change_address(string(), boolean()) -> ok | not_renamed |
                                                {address_save_failed, _} |
                                                not_self_started.
do_change_address(NewAddr, UserSupplied) ->
    do_change_address(NewAddr, UserSupplied, undefined).

-spec do_change_address(string(), boolean(), fun() | undefined) ->
                                                ok | not_renamed |
                                                {address_save_failed, _} |
                                                not_self_started |
                                                {validation_failed, any()}.
do_change_address(NewAddr, UserSupplied, AddrValidationFun) ->
    NewAddr1 =
        case UserSupplied of
            false ->
                misc:get_env_default(rename_ip, NewAddr);
            true ->
                NewAddr
        end,

    ?cluster_info("Change of address to ~p is requested.", [NewAddr1]),
    case maybe_rename(NewAddr1, UserSupplied, AddrValidationFun) of
        not_renamed ->
            not_renamed;
        renamed ->
            ?cluster_info("Renamed node. New name is ~p.", [node()]),
            ok;
        Other ->
            Other
    end.

maybe_rename(NewAddr, UserSupplied, AddrValidationFun) ->
    OldName = node(),
    OnRename =
        fun() ->
                %% prevent node disco events while we're in the middle
                %% of renaming
                ns_node_disco:register_node_renaming_txn(self()),

                %% prevent breaking remote monitors while we're in the middle
                %% of renaming
                remote_monitors:register_node_renaming_txn(self())
        end,

    case dist_manager:adjust_my_address(NewAddr, UserSupplied, OnRename,
                                        AddrValidationFun) of
        nothing ->
            ?cluster_debug("Not renaming node.", []),
            not_renamed;
        not_self_started ->
            ?cluster_debug("Didn't rename the node because net_kernel "
                           "is not self started", []),
            not_self_started;
        {address_save_failed, _} = Error ->
            Error;
        net_restarted ->
            ?cluster_debug("Renamed node from ~p to ~p.", [OldName, node()]),
            renamed;
        {validation_failed, _} = Error ->
            Error
    end.

check_add_possible(RemoteAddr, Body) ->
    EnsureProvisioned =
        fun () ->
            case ns_config_auth:is_system_provisioned() of
                true -> ok;
                false ->
                    Msg = <<"Adding nodes to not provisioned nodes is not "
                            "allowed.">>,
                    {error, system_not_provisioned, Msg}
            end
        end,
    NoRebalanceRunning =
        fun () ->
            case rebalance:running() of
                false -> ok;
                true ->
                    Msg = <<"Node addition is disallowed while rebalance "
                            "is in progress">>,
                    {error, rebalance_running, Msg}
            end
        end,
    CheckHostAllowed =
        fun () ->
            AllowedHosts = allowed_hosts(),
            ?log_info("Checking if host '~s' is allowed to join the cluster "
                      "(allowed hosts: ~1000p)", [RemoteAddr, AllowedHosts]),
            case is_host_allowed(RemoteAddr, AllowedHosts) of
                true -> ok;
                false ->
                    Msg = io_lib:format(
                            "Host ~s is not allowed to join. Check "
                            "allowedHosts setting.", [RemoteAddr]),
                    {error, not_allowed_host, iolist_to_binary(Msg)}
            end
        end,
    case functools:sequence_([EnsureProvisioned, NoRebalanceRunning,
                              CheckHostAllowed]) of
        ok -> Body();
        Error -> Error
    end.

do_add_node(Scheme, RemoteAddr, RestPort, HiddenAuth, GroupUUID, Services) ->
    check_add_possible(
      RemoteAddr,
      fun () ->
              do_add_node_allowed(Scheme, RemoteAddr, RestPort, HiddenAuth,
                                  GroupUUID, Services)
      end).

should_change_address() ->
    %% adjust our name if we're alone
    ns_node_disco:nodes_wanted() =:= [node()] andalso
        not dist_manager:using_user_supplied_address().

do_add_node_allowed(Scheme, RemoteAddr, RestPort, HiddenAuth, GroupUUID, Services) ->
    AFamily = misc:get_net_family(),
    case check_host_port_connectivity(RemoteAddr, RestPort, AFamily) of
        {ok, MyIP} ->
            R = case should_change_address() of
                    true ->
                        ValidationFun =
                            fun (Addr) ->
                                case is_host_allowed(Addr) of
                                    true -> ok;
                                    false -> {error, {not_allowed, Addr}}
                                end
                            end,
                        case do_change_address(MyIP, false, ValidationFun) of
                            {address_save_failed, _} = E ->
                                E;
                            {validation_failed, _} = E ->
                                E;
                            not_self_started ->
                                ?cluster_debug("Haven't changed address because of not_self_started condition", []),
                                ok;
                            not_renamed ->
                                ok;
                            ok ->
                                ok
                        end;
                    false ->
                        ok
                end,
            case R of
                ok ->
                    do_add_node_with_cert_validation(Scheme, RemoteAddr,
                                                     RestPort, HiddenAuth,
                                                     GroupUUID, Services);
                {validation_failed, {not_allowed, Addr}} ->
                    Msg = io_lib:format(
                            "Could not use address '~s' for the node that is "
                            "already part of the cluster because it doesn't "
                            "seem to be allowed FQDN/IP. "
                            "Check allowedHosts setting", [Addr]),
                    {error, rename_failed, iolist_to_binary(Msg)};
                {address_save_failed, Error} ->
                    Msg = io_lib:format(
                            "Could not save address after rename: ~p", [Error]),
                    {error, rename_failed, iolist_to_binary(Msg)}
            end;
        {error, Reason} ->
            M1 = case ns_error_messages:connection_error_message(
                        Reason, RemoteAddr, RestPort) of
                    undefined -> io_lib:format("~p", [Reason]);
                    Msg -> Msg
                end,
            M2 = ns_error_messages:engage_cluster_error({engage_cluster_failed,
                                                         {"cluster", Reason,
                                                          M1, RemoteAddr,
                                                          {AFamily}
                                                         }}),
            URL = menelaus_rest:rest_url(RemoteAddr, RestPort, "", Scheme),
            ReasonStr = io_lib:format("Failed to connect to ~s. ~s",
                                      [URL, M2]),
            {error, host_connectivity, iolist_to_binary(ReasonStr)}
    end.

do_add_node_with_cert_validation(Scheme, RemoteAddr, RestPort, HiddenAuth,
                                 GroupUUID, Services) ->
    case validate_local_node_certificate_san() of
        ok ->
            do_add_node_with_connectivity(Scheme, RemoteAddr, RestPort,
                HiddenAuth, GroupUUID, Services);
        {error, _, _} = Err -> Err
    end.

validate_local_node_certificate_san() ->
    % Only check certificate SAN when cluster has only
    % one node. This will prevent failure when
    % adding nodes to a 2+ node cluster with invalid
    % certificates which can happen often when cluster
    % is probably upgraded from prev versions and is
    % working properly. At the same time, extracting certificate is only valid
    % where the enterprise version is running not the community version.
    % Single node cluster may have their named changed, that's why we allow
    % such clusters with invalid certs to be initialized and
    % later when adding another node we will fail
    % if the cert is invalid.
    CertIsNotOK =
        case
            length(ns_cluster_membership:nodes_wanted()) == 1 andalso
            cluster_compat_mode:is_enterprise() of
            true ->
                NodeCertProps = ns_server_cert:get_cert_info(node_cert, node()),
                ns_server_cert:verify_cert_hostname_strict(node(),
                                                           NodeCertProps) /= ok;
            false -> false
        end,

    if
        CertIsNotOK ->
            HostName = misc:extract_node_address(node()),
            ?cluster_debug("Node certificate doesn't include node's name: ~p",
                [HostName]),
            Msg =
                ns_error_messages:reload_node_certificate_error(
                    {bad_server_cert_san, HostName}),
            {error, invalid_cert, Msg};
        true ->
            ok
    end.

post_json_to_joinee(Target, HiddenAuth, Options, Stuff) ->
    ?cluster_debug("Posting the following to ~p:~n~p",
                   [Target, {sanitize_node_info(Stuff)}]),
    Post = list_to_tuple(Target ++
                             ["application/json", mochijson2:encode(Stuff)]),
    RV = menelaus_rest:json_request_hilevel(post, Post,
                                            HiddenAuth,
                                            Options),
    ?cluster_debug("Reply from ~p:~n~p", [Target, sanitize_node_info(RV)]),

    case RV of
        {client_error, [Message]} when is_binary(Message) ->
            {error, Message};
        {client_error, _} ->
            {error, invalid_json};
        {error, rest_error, M,
         {error, {{tls_alert, {certificate_required, _}}, _}}} ->
            Msg = io_lib:format("Cluster node requires per-node client "
                                "certificate when client certificate "
                                "authentication is set to mandatory. ~s", [M]),
            {error, iolist_to_binary(Msg)};
        {error, rest_error, M,
         {error, {{tls_alert, {unknown_ca, _}} = E, _}}} ->
            {error, ns_error_messages:engage_cluster_error(
                      {engage_cluster_failed,
                       {"cluster", E, M, node(), {}}})};
        {error, _, X, _} ->
            {error, X};
        Other ->
            Other
    end.

engage_cluster_bad_json_error(Exc) ->
    {error, engage_cluster_bad_json,
     ns_error_messages:engage_cluster_error({json, Exc})}.

do_add_node_with_connectivity(Scheme, RemoteAddr, RestPort, HiddenAuth,
                              GroupUUID, Services) ->
    {NodeInfo} = menelaus_web_node:build_full_node_info(node()),
    IsPreviewCluster = cluster_compat_mode:is_developer_preview(),
    Config = ns_config:get(),

    IsEnterprise = cluster_compat_mode:is_enterprise(Config),

    CertProps = case IsEnterprise of
                    true -> engage_cluster_certs_props(Config, RemoteAddr);
                    false -> []
                end,

    Props = [{<<"requestedTargetNodeHostname">>, list_to_binary(RemoteAddr)},
             {<<"requestedServices">>, Services},
             {<<"isDeveloperPreview">>, IsPreviewCluster}]
        ++ NodeInfo ++ CertProps,

    GeneratedCerts = ns_server_cert:this_node_uses_self_generated_certs(Config),
    Options = [{connect_options, [misc:get_net_family()]},
               {server_verification, not GeneratedCerts}],

    MustUseHttps =
        (not ns_config:read_key_fast(allow_http_node_addition, false)) and
        %% We don't want to pass generated key unencrypted
        (proplists:is_defined(<<"autogeneratedKey">>, Props) or IsEnterprise),

    AllowedScheme = case MustUseHttps of
                        true -> Scheme == https;
                        false -> true
                    end,

    case AllowedScheme of
        true ->
            case post_json_to_joinee(
                   [Scheme, RemoteAddr, RestPort, "/engageCluster2"],
                   HiddenAuth, Options, {Props}) of
                {ok, {NodeKVList}} ->
                    try
                        do_add_node_engaged(NodeKVList, HiddenAuth, GroupUUID,
                                            Services, Scheme)
                    catch
                        exit:{unexpected_json, _Where, _Field} = Exc ->
                            engage_cluster_bad_json_error(Exc)
                    end;
                {ok, _JSON} ->
                    engage_cluster_bad_json_error(undefined);
                {error, invalid_json} ->
                    engage_cluster_bad_json_error(undefined);
                {error, Msg} ->
                    M = iolist_to_binary([<<"Prepare join failed. ">>, Msg]),
                    {error, engage_cluster, M}
            end;
        false ->
            {error, not_allowed_scheme,
             <<"http is prohibited due to security reasons, please use https">>}
    end.

engage_cluster_certs_props(Config, RemoteAddr) ->
    GenerateCerts =
        fun (Type, Arg) ->
            case ns_server_cert:generate_certs(Type, Arg) of
                no_private_key ->
                    _ = ns_server_cert:generate_cluster_CA(true, false),
                    ns_server_cert:generate_certs(Type, Arg);
                Certs -> Certs
            end
        end,

    NodeCerts =
        case ns_server_cert:this_node_uses_self_generated_certs(Config) of
            true ->
                {CA, NewNodeCert, NewNodeKey} =
                    GenerateCerts(node_cert, RemoteAddr),
                %% Sending PKey only in case if this cluster uses
                %% autogenerated certs (otherwise user is supposed to provision
                %% new node with certs)
                [{<<"autogeneratedCA">>, CA},
                 {<<"autogeneratedCert">>, NewNodeCert},
                 %% otpCookie is a trigger to sanitize this key before it is
                 %% logged. The wrapping will always be required to handle
                 %% the case where older releases, even those no longer
                 %% supported, attempt to connect to us. Without the wrapping
                 %% those older releases will log an unsanitized key.
                 {<<"autogeneratedKey">>, {[{otpCookie, NewNodeKey}]}}];
            false -> []
        end,

    ClientCerts =
        case ns_server_cert:this_node_uses_self_generated_client_certs(Config) of
            true ->
                {CA2, NewClientCert, NewClientKey} =
                    GenerateCerts(client_cert, ?INTERNAL_CERT_USER),
                [{<<"autogeneratedCA">>, CA2},
                 {<<"autogeneratedClientCert">>, NewClientCert},
                 %% otpCookie is a trigger to sanitize this key before it is
                 %% logged (see the note above for additional info).
                 {<<"autogeneratedClientKey">>, {[{otpCookie, NewClientKey}]}}];
            false -> []
        end,
    misc:update_proplist(NodeCerts, ClientCerts).

expect_json_property_base(PropName, KVList) ->
    case lists:keyfind(PropName, 1, KVList) of
        false ->
            erlang:exit({unexpected_json, missing_property, PropName});
        Tuple -> element(2, Tuple)
    end.

expect_json_property_binary(PropName, KVList) ->
    RV = expect_json_property_base(PropName, KVList),
    case is_binary(RV) of
        true -> RV;
        _ -> erlang:exit({unexpected_json, not_binary, PropName})
    end.

expect_json_property_list(PropName, KVList) ->
    binary_to_list(expect_json_property_binary(PropName, KVList)).

expect_json_property_atom(PropName, KVList) ->
    binary_to_atom(expect_json_property_binary(PropName, KVList), latin1).

expect_json_property_integer(PropName, KVList) ->
    RV = expect_json_property_base(PropName, KVList),
    expect_integer(PropName, RV).

expect_integer(PropName, Value) ->
    case is_integer(Value) of
        true -> Value;
        false -> erlang:exit({unexpected_json, not_integer, PropName})
    end.

get_port_from_epmd(OtpNode, AFamily, Encryption) ->
    Res = case cb_epmd:get_port(OtpNode, AFamily, Encryption)  of
              {port, Port, _Version} -> {ok, Port};
              noport -> {error, noport};
              {error, _} = Error -> Error
          end,
    ?cluster_debug("port_please(~p, ~p, ~p) = ~p",
                   [OtpNode, AFamily, Encryption, Res]),
    Res.

verify_otp_connectivity(OtpNode, Options) ->
    NodeAFamily = proplists:get_value(node_afamily, Options,
                                      cb_dist:address_family()),
    NodeEncryption = proplists:get_value(node_encryption, Options,
                                         cb_dist:external_encryption()),
    Host = misc:extract_node_address(OtpNode, NodeAFamily),
    PortRes =
        case proplists:get_value(port, Options) of
            undefined ->
                get_port_from_epmd(OtpNode, NodeAFamily, NodeEncryption);
            P ->
                {ok, P}
        end,
    case PortRes of
        {ok, Port} ->
            VerifyConnectivity =
                case NodeEncryption of
                    true ->
                        check_otp_tls_connectivity(_, _, _, Options);
                    false ->
                        fun check_host_port_connectivity/3
                end,
            case VerifyConnectivity(Host, Port, NodeAFamily) of
                {ok, IP} -> {ok, IP};
                {error, Reason} ->
                    {error, connect_node,
                     {Reason,
                      ns_error_messages:verify_otp_connectivity_connection_error(
                        Reason, OtpNode, Host, Port)}}
            end;
        Error ->
            {error, connect_node,
             {Error,
              ns_error_messages:verify_otp_connectivity_port_error(OtpNode,
                                                                   Host,
                                                                   Error)}}
    end.

check_otp_tls_connectivity(Host, Port, AFamily, Options) ->
    %% Building connect options the same way inet_tls_dist builds it.
    %% Note that ssl_dist_opts ets is populated by erlang ssl app (in our case
    %% it should contain options from /etc/ssl_dist_opts.in).
    [{client, TLSOpts}] = ets:lookup(ssl_dist_opts, client),
    Opts = application:get_env(kernel, inet_dist_connect_options, []),
    %% cb_dist might not have updated the password in ssl_dist_opts yet,
    %% so update it here
    PassFun = ns_secrets:get_pkey_pass(client_cert),
    Opts2 = misc:update_proplist(Opts, [{password, PassFun}]),
    SNIOpts = case inet:parse_address(Host) of
                  {ok, _} -> [];
                  _ -> [{server_name_indication, Host}]
              end,
    AllOpts = [binary, {active, false}, {packet, 4},
               AFamily, {nodelay, true}, {erl_dist, true}] ++ SNIOpts ++
               lists:ukeysort(1, Opts2 ++ TLSOpts),
    AllOpts2 =
        lists:foldl(
          fun ({cert, Cert}, Acc) ->
                  [{cert, Cert} |
                   proplists:delete(certfile, proplists:delete(cert, Acc))];
              ({key, Key}, Acc) ->
                  [{key, Key} | functools:chain(
                                  Acc, [proplists:delete(key, _),
                                        proplists:delete(keyfile, _),
                                        proplists:delete(password, _)])];
              (_, Acc) ->
                  Acc
          end, AllOpts, Options),
    Timeout = net_kernel:connecttime(),
    try
        IpAddr =
            case inet:getaddr(Host, AFamily) of
                {ok, A} -> A;
                {error, _} = Error0 -> throw(Error0)
            end,

        TLSSocket =
            case ssl:connect(IpAddr, Port, AllOpts2, Timeout) of
                {ok, S} -> S;
                {error, _} = Error1 -> throw(Error1)
            end,

        %% Work around for a bug where ssl:connect retuns ok in case when
        %% the client cert is incorrect (TLS 1.3 only). Seems like the alert
        %% arrives immediatelly after the handshake, hence the timeout.
        %% In case if cert is ok, this function always waits for 1 second
        %% unfortunatelly.
        case ssl:connection_information(TLSSocket, [protocol]) of
            {ok, [{protocol, 'tlsv1.3'}]} ->
                case ssl:recv(TLSSocket, 1, 1000) of
                    {error, {tls_alert, _} = Alert} -> throw({error, Alert});
                    {error, closed} -> throw({error, unknown});
                    _ -> ok
                end;
            _ -> ok
        end,

        LocalIpAddr =
            case ssl:sockname(TLSSocket) of
                {ok, {Addr, _}} ->
                    catch ssl:close(TLSSocket),
                    Addr;
                {error, _} = Error2 ->
                    ?log_error("Established test connecton but could "
                               "not extract sockname: ~p", [Error2]),
                    throw(Error2)
            end,

        ?log_debug("Successfully checked OTP TLS connectivity to ~p(~p):~p",
                   [Host, IpAddr, Port]),

        {ok, inet:ntoa(LocalIpAddr)}

    catch
        throw:{error, Reason} ->
            {error, Reason}
    end.

do_add_node_engaged(NodeKVList, HiddenAuth, GroupUUID, Services, Scheme) ->
    OtpNode = expect_json_property_atom(<<"otpNode">>, NodeKVList),

    RV = verify_otp_connectivity(OtpNode, []),
    case RV of
        {ok, _} ->
            case check_can_add_node(NodeKVList) of
                ok ->
                    RequestTarget = get_request_target(NodeKVList, Scheme),
                    %% TODO: only add services if possible
                    %% TODO: consider getting list of supported
                    %% services from NodeKVList
                    node_add_transaction(
                      OtpNode, GroupUUID, Services,
                      do_add_node_engaged_inner(
                        _, RequestTarget, OtpNode, HiddenAuth, Services));
                Error -> Error
            end;
        {error, connect_node, {Err, Msg}} ->
            {error, connect_node,
             ns_error_messages:engage_cluster_error({engage_cluster_failed,
                                                     {"cluster", Err, Msg,
                                                      node(), {}}})}
    end.

check_can_add_node(NodeKVList) ->
    JoineeClusterCompatVersion = expect_json_property_integer(<<"clusterCompatibility">>, NodeKVList),
    JoineeNode = expect_json_property_atom(<<"otpNode">>, NodeKVList),

    MyCompatVersion = cluster_compat_mode:effective_cluster_compat_version(),
    case JoineeClusterCompatVersion =:= MyCompatVersion of
        true ->
            ok;
        false ->
            {error, incompatible_cluster_version,
             ns_error_messages:incompatible_cluster_version_error(
               MyCompatVersion, JoineeClusterCompatVersion, JoineeNode)}
    end.

get_request_target(NodeKVList, Scheme) ->
    HostnameRaw = expect_json_property_list(<<"hostname">>, NodeKVList),
    {Hostname, NonHttpsPort} = misc:split_host_port(HostnameRaw, "8091",
                                                     misc:get_net_family()),
    {Ports} = proplists:get_value(<<"ports">>, NodeKVList, {[]}),
    {Scheme2, Port} =
        case proplists:get_value(<<"httpsMgmt">>, Ports) of
            undefined -> {http, NonHttpsPort};
            P when Scheme == https -> {https, P};
            _ -> {http, NonHttpsPort}
        end,
    {Scheme2, Hostname, Port}.

do_add_node_engaged_inner(ChronicleInfo, {Scheme, Hostname, Port},
                          OtpNode, HiddenAuth, Services) ->
    {MyNodeKVList} =
        menelaus_web_node:build_full_node_info(undefined, node(), true),
    Struct = {[{<<"targetNode">>, OtpNode},
               {<<"requestedServices">>, Services},
               {<<"chronicleInfo">>,
                base64:encode(term_to_binary(ChronicleInfo))}
              | MyNodeKVList]},

    Options = [{connect_options, [misc:get_net_family()]},
               {timeout, ?COMPLETE_TIMEOUT},
               {connect_timeout, ?COMPLETE_TIMEOUT},
               {server_verification, true}],

    case post_json_to_joinee([Scheme, Hostname, Port, "/completeJoin"],
                              HiddenAuth, Options, Struct) of
        {ok, _} ->
            {ok, OtpNode};
        {error, Msg} ->
            {error, complete_join,
            case Msg of
                invalid_json ->
                    <<"REST call returned invalid json.">>;
                _ ->
                    iolist_to_binary([<<"Join completion call failed. ">>, Msg])
            end}
    end.

node_add_transaction(Node, GroupUUID, Services, Body) ->
    case chronicle_master:add_replica(Node, GroupUUID, Services) of
        {ok, ChronicleInfo} ->
            node_add_transaction_finish(Node, GroupUUID, ChronicleInfo, Body);
        cannot_acquire_lock ->
            {error, cannot_acquire_lock,
             <<"Operation temporarily cannot be performed possibly due to loss "
               "of quorum">>};
        group_not_found ->
            M = iolist_to_binary([<<"Could not find group with uuid: ">>,
                                  GroupUUID]),
            {error, unknown_group, M};
        node_present ->
            M = iolist_to_binary([<<"Node already exists in cluster: ">>,
                                  atom_to_list(Node)]),
            {error, node_present, M};
        unfinished_failover ->
            {error, unfinished_failover,
             <<"Operation cannot be performed due to unfinished failover">>}
    end.

node_add_transaction_finish(Node, GroupUUID, ChronicleInfo, Body) ->
    ?cluster_info("Started node add transaction by adding node ~p to nodes_wanted (group: ~s)",
                  [Node, GroupUUID]),
    try Body(ChronicleInfo) of
        {ok, _} = X -> X;
        Crap ->
            ?cluster_error("Add transaction of ~p failed because of ~p",
                           [Node, Crap]),
            shun(Node),
            Crap
    catch
        Type:What:Stack ->
            ?cluster_error("Add transaction of ~p failed because of exception ~p",
                           [Node, {Type, What, Stack}]),
            shun(Node),
            erlang:Type(What),
            erlang:error(cannot_happen)
    end.

do_engage_cluster(NodeKVList) ->
    try
        do_engage_cluster_check_compatibility(NodeKVList)
    catch
        exit:{unexpected_json, _, _} = Exc ->
            {error, unexpected_json,
             ns_error_messages:engage_cluster_error({json, Exc})}
    end.

do_engage_cluster_check_compatibility(NodeKVList) ->
    Version = expect_json_property_binary(<<"version">>, NodeKVList),
    Node = expect_json_property_atom(<<"otpNode">>, NodeKVList),
    %% Prior to 7.6.2, this used to reject on a version of 1.* as too old, but
    %% since the initial version of Columnar is 1.0.0, update this validation
    %% to only fail if a the clusterCompatibility property is absent, to allow
    %% Columnar nodes to join.
    case lists:keyfind(<<"clusterCompatibility">>, 1, NodeKVList) of
        false ->
            case Version of
                <<"1.",_/binary>> ->
                    {error, incompatible_cluster_version,
                     ns_error_messages:too_old_version_error(Node, Version)};
                _ ->
                    erlang:exit({unexpected_json, missing_property,
                                 <<"clusterCompatibility">>})
            end;
        _ ->
            do_engage_cluster_check_compat_version(Node, Version, NodeKVList)
    end.

do_engage_cluster_check_compat_version(Node, Version, NodeKVList) ->
    ActualCompatibility = expect_json_property_integer(<<"clusterCompatibility">>, NodeKVList),
    MinSupportedCompatVersion = cluster_compat_mode:min_supported_compat_version(),
    MinSupportedCompatibility =
        cluster_compat_mode:effective_cluster_compat_version_for(MinSupportedCompatVersion),
    IsPreviewCluster =
        proplists:get_value(<<"isDeveloperPreview">>, NodeKVList, false),
    SupportedVsn = cluster_compat_mode:supported_compat_version(),
    WantedCompatibility =
        cluster_compat_mode:effective_cluster_compat_version_for(SupportedVsn),

    if
        ActualCompatibility < MinSupportedCompatibility ->
            {error, incompatible_cluster_version,
             ns_error_messages:too_old_version_error(Node, Version)};
        IsPreviewCluster and (WantedCompatibility > ActualCompatibility) ->
            {error, incompatible_cluster_version,
             ns_error_messages:preview_cluster_join_error()};
        true ->
            do_engage_cluster_check_services(NodeKVList)
    end.

get_requested_services(KVList) ->
    Default = ns_cluster_membership:default_services(),
    do_get_requested_services(<<"requestedServices">>, KVList, Default).

do_get_requested_services(Key, KVList, Default) ->
    case lists:keyfind(Key, 1, KVList) of
        false ->
            {ok, Default};
        {_, List} ->
            case is_list(List) of
                false ->
                    erlang:exit({unexpected_json, not_list, Key});
                _ ->
                    case [[] || B <- List, not is_binary(B)] =:= [] of
                        false ->
                            erlang:exit({unexpected_json, not_list, Key});
                        true ->
                            try
                                {ok, [binary_to_existing_atom(S, latin1) || S <- List]}
                            catch
                                error:badarg ->
                                    {bad_services, List}
                            end
                    end
            end
    end.

community_allowed_topologies() ->
    KvOnly = [kv],
    AllServices40 = [kv, index, n1ql],
    AllServices = ns_cluster_membership:supported_services(false),

    [KvOnly, lists:sort(AllServices40), lists:sort(AllServices)].

enforce_topology_limitation(Services) ->
    case cluster_compat_mode:is_enterprise() of
        true ->
            ok;
        false ->
            SortedServices = lists:sort(Services),
            SupportedCombinations = community_allowed_topologies(),
            case lists:member(SortedServices, SupportedCombinations) of
                true ->
                    ok;
                false ->
                    {error, ns_error_messages:topology_limitation_error(SupportedCombinations)}
            end
    end.

do_engage_cluster_check_services(NodeKVList) ->
    SupportedServices = ns_cluster_membership:supported_services(),
    case get_requested_services(NodeKVList) of
        {ok, Services} ->
            case Services -- SupportedServices of
                [] ->
                    case enforce_topology_limitation(Services) of
                        {error, Msg} ->
                            {error, incompatible_services, Msg};
                        ok ->
                            do_engage_cluster_inner(NodeKVList, Services)
                    end;
                _ ->
                    unsupported_services_error(SupportedServices, Services)
            end;
        {bad_services, RawServices} ->
            unsupported_services_error(SupportedServices, RawServices)
    end.

unsupported_services_error(Supported, Requested) ->
    {error, incompatible_services,
     ns_error_messages:unsupported_services_error(Supported, Requested)}.

do_engage_cluster_inner(NodeKVList, Services) ->
    OtpNode = expect_json_property_atom(<<"otpNode">>, NodeKVList),
    MaybeTargetHost = proplists:get_value(<<"requestedTargetNodeHostname">>, NodeKVList),
    case verify_otp_connectivity(OtpNode, []) of
        {ok, MyIP} ->
            {Address, UserSupplied} =
                case MaybeTargetHost of
                    undefined ->
                        {MyIP, false};
                    _ ->
                        TargetHost = binary_to_list(MaybeTargetHost),
                        {TargetHost, true}
                end,

            case do_engage_cluster_inner_check_address(Address, UserSupplied) of
                ok ->
                    do_engage_cluster_inner_tail(NodeKVList, Address, UserSupplied, Services);
                Error ->
                    Error
            end;
        {error, connect_node, {Err, Msg}} ->
            {error, connect_node,
             ns_error_messages:engage_cluster_error({engage_cluster_failed,
                                                     {"new node", Err, Msg,
                                                      node(), {}}})}
    end.

do_engage_cluster_inner_check_address(_Address, false) ->
    ok;
do_engage_cluster_inner_check_address(Address, true) ->
    case misc:is_good_address(Address) of
        ok ->
            ok;
        {ErrorType, _} = Error ->
            Msg = ns_error_messages:address_check_error(Address, Error),
            {error, ErrorType, Msg}
    end.

do_engage_cluster_inner_tail(NodeKVList, Address, UserSupplied, Services) ->
    case do_change_address(Address, UserSupplied) of
        {address_save_failed, Error1} ->
            Msg = io_lib:format("Could not save address after rename: ~p",
                                [Error1]),
            {error, rename_failed, iolist_to_binary(Msg)};
        Res when Res =:= ok;
                 Res =:= not_renamed;
                 Res =:= not_self_started ->
            do_engage_cluster_validate_cert(NodeKVList, Services)
    end.

do_engage_cluster_validate_cert(NodeKVList, Services) ->
    case validate_local_node_certificate_san() of
        ok ->
            %% we re-init node's cookie to support joining cloned
            %% nodes. If we don't do that cluster will be able to
            %% connect to this node too soon. And then initial set of
            %% nodes_wanted by node thats added to cluster may
            %% 'pollute' cluster's version and cause issues. See
            %% MB-4476 for details.
            ns_cookie_manager:cookie_init(),
            check_can_join_to(NodeKVList, Services);
        {error, _, _} = Err -> Err
    end.

json_field(Service) ->
    list_to_binary(atom_to_list(memory_quota:service_to_json_name(Service))).

check_memory_size(NodeKVList, Services) ->
    Quotas =
        lists:foldl(
          fun(kv, Acc) ->
                  Quota = expect_json_property_integer(json_field(kv), NodeKVList),
                  [{kv, Quota} | Acc];
             (Service, Acc) ->
                  JSONField = json_field(Service),
                  case lists:keyfind(JSONField, 1, NodeKVList) of
                      {_, Quota} ->
                          [{Service, expect_integer(JSONField, Quota)} | Acc];
                      false ->
                          Acc
                  end
          end, [], memory_quota:aware_services()),

    case memory_quota:check_this_node_quotas(Services, Quotas) of
        ok ->
            ok;
        {error, {total_quota_too_high, Node, TotalQuota, MaxQuota}} ->
            {error, bad_memory_size,
             ns_error_messages:bad_memory_size_error(Services, TotalQuota,
                                                     MaxQuota, Node)}
    end.

check_can_join_to(NodeKVList, Services) ->
    case check_memory_size(NodeKVList, Services) of
        ok -> {ok, ok};
        Error -> Error
    end.

get_chronicle_info(KVList) ->
    binary_to_term(base64:decode(
                     proplists:get_value(<<"chronicleInfo">>, KVList))).

-spec do_complete_join([{binary(), term()}]) ->
          {ok, ok} | {error, atom() | {actual_join_failed, atom()}, binary()}.
do_complete_join(NodeKVList) ->
    try
        OtpNode = expect_json_property_atom(<<"otpNode">>, NodeKVList),
        OtpCookie = expect_json_property_atom(<<"otpCookie">>, NodeKVList),
        MyNode = expect_json_property_atom(<<"targetNode">>, NodeKVList),

        {ok, Services} = get_requested_services(NodeKVList),
        case check_can_join_to(NodeKVList, Services) of
            {ok, _} ->
                case ns_cluster_membership:system_joinable() andalso MyNode =:= node() of
                    false ->
                        {error, join_race_detected,
                         <<"Node is already part of cluster.">>};
                    true ->
                        perform_actual_join(
                          OtpNode, OtpCookie, get_chronicle_info(NodeKVList))
                end;
            Error -> Error
        end
    catch exit:{unexpected_json, _Where, _Field} ->
            {error, engage_cluster_bad_json,
             ns_error_messages:engage_cluster_error({json, undefined})}
    end.


perform_actual_join(RemoteNode, NewCookie, ChronicleInfo) ->
    maybe
        ?cluster_log(0002, "Node ~p is joining cluster via node ~p.",
                    [node(), RemoteNode]),
        %% let ns_memcached know that we don't need to preserve data at all
        ns_config:set(i_am_a_dead_man, true),

        %% Pull the rug out from under the app
        misc:create_marker(start_marker_path()),
        ok = ns_server_cluster_sup:stop_ns_server(),
        ns_log:delete_log(),

        ?cluster_debug("ns_cluster: joining cluster. Child has exited.", []),
        misc:create_marker(join_marker_path()),

        menelaus_users:delete_storage_offline(),
        ns_cluster_membership:prepare_to_join(RemoteNode, NewCookie),

        ok = chronicle_local:prepare_join(ChronicleInfo),

        %% reload is needed to reinitialize ns_config's cache after
        %% config cleanup ('erase' causes the problem, but it looks like
        %% it's not worth it to add proper 'erase' support to ns_config)
        ns_config:reload(),
        ns_config:merge_dynamic_and_static(),
        ?cluster_debug("pre-join cleaned config is:~n~p",
                    [ns_config_log:sanitize(ns_config:get())]),

        {ok, _Cookie} = ns_cookie_manager:cookie_sync(),
        %% Let's verify connectivity.

        Connected =
            misc:poll_for_condition(
            fun () ->
                    ?log_debug("Trying to connect to node ~p...", [RemoteNode]),
                    case catch net_kernel:connect_node(RemoteNode) of
                        true -> true;
                        Res ->
                            ?log_error("Connect to node ~p failed: ~p",
                                        [RemoteNode, Res]),
                            false
                    end
            end, 10000, 500),

        ?cluster_debug("Connection from ~p to ~p:  ~p",
                    [node(), RemoteNode, Connected]),

        ok = chronicle_local:join_cluster(ChronicleInfo),

        {_, ok} ?= {reencrypt_deks, cb_cluster_secrets:reencrypt_deks()},
        encryption_service:revalidate_key_cache(),

        %% Make sure that latest timestamps are published synchronously.
        tombstone_agent:refresh(),

        ok = ns_config_rep:pull_from_one_node_directly(RemoteNode),
        ?cluster_debug("pre-join merged config is:~n~p",
                    [ns_config_log:sanitize(ns_config:get())]),

        %% New cluster's epoch overwrites "old" cluster's epoch,
        %% but this node cert still contains epoch from the "old" cluster,
        %% which is wrong. Reset it to the value from new cluster.
        ns_ssl_services_setup:update_certs_epoch(),

        misc:remove_marker(join_marker_path()),

        ?cluster_debug("Join succeded, starting ns_server_cluster back", []),
        {_, ok} ?= {start_server, ns_server_cluster_sup:start_ns_server()},
        misc:remove_marker(start_marker_path()),
        ?cluster_log(?NODE_JOINED, "Node ~s joined cluster", [node()]),
        {ok, ok}
    else
        {Type, {error, Error}} ->
            ?cluster_error("Failed to join cluster because of: ~p", [Error]),
            {ErrorTerm, ErrorMsg} = format_actual_join_error(Type, Error),
            {error, {actual_join_failed, ErrorTerm}, ErrorMsg}
    end.

format_actual_join_error(start_server, _Error) ->
    {start_cluster_failed,
     <<"Failed to start ns_server cluster processes back. "
       "Logs might have more details.">>};
format_actual_join_error(reencrypt_deks, _Error) ->
    {reencrypt_deks_failed,
     <<"Failed to reencrypt some encryption-at-rest keys. "
       "Logs might have more details.">>}.

perform_leave() ->
    chronicle_local:leave_cluster(),

    menelaus_users:delete_storage_offline(),
    prometheus_cfg:wipe(),
    ns_ssl_services_setup:remove_node_certs(),

    %% in order to disconnect from rest of nodes we need new cookie
    %% and explicit disconnect_node calls
    {ok, _} = ns_cookie_manager:cookie_init(),

    %% reset_address() below drops user_assigned flag (if any) which makes
    %% it possible for the node to be renamed if necessary
    ok = dist_manager:reset_address(),
    %% and then we clear config. In fact better name would be 'reset',
    %% because as seen above we actually re-initialize default config
    tombstone_agent:wipe(),

    %% Remove the persisted lease from disk if it exists. It holds no relevance
    %% after node has left the cluster
    leader_lease_agent:cleanup_lease(),

    ns_config:clear([directory,
                     %% Preserve these directories as they may have been
                     %% changed from their defaults and their handling
                     %% should be consistent with the way we retain the
                     %% index and data directories.
                     {node, node(), database_dir},
                     {node, node(), index_dir},
                     {node, node(), cbas_dirs},
                     {node, node(), eventing_dir},
                     %% we preserve rest settings, so if the server runs on a
                     %% custom port, it doesn't revert to the default
                     rest,
                     {node, node(), rest},
                     {node, node(), address_family},
                     {node, node(), address_family_only},
                     {node, node(), node_encryption},
                     {node, node(), erl_external_listeners},
                     {node, node(), n2n_client_cert_auth}]),


    %% set_initial here clears vclock on nodes_wanted. Thus making
    %% sure that whatever nodes_wanted we will get through initial
    %% config replication (right after joining cluster next time) will
    %% not conflict with this value.
    ns_config:set_initial(nodes_wanted, [node()]),
    {ok, _} = ns_cookie_manager:cookie_sync(),
    ok = cb_cluster_secrets:reencrypt_deks(),
    encryption_service:revalidate_key_cache().

leave_marker_path() ->
    path_config:component_path(data, "leave_marker").

start_marker_path() ->
    path_config:component_path(data, "start_marker").

rename_marker_path() ->
    path_config:component_path(data, "rename_marker").

join_marker_path() ->
    path_config:component_path(data, "join_marker").

allowed_hosts() ->
    ns_config:read_key_fast(allowed_hosts, [<<"*">>]).

is_host_allowed(Host) when is_list(Host) ->
    is_host_allowed(Host, allowed_hosts()).

is_host_allowed(Host, AllowedHosts) ->
    host_match_any(Host, [menelaus_web_settings:parse_allowed_host(AH)
                          || AH <- AllowedHosts]).

host_match_any(Host, AllowedHostsParsed) ->
    lists:any(fun (Pattern) -> host_match(Host, Pattern) end,
              AllowedHostsParsed).

host_match(_, any) ->
    true;
host_match(Host, {ip, IP}) ->
    case inet:parse_address(Host) of
        {ok, IP} -> true;
        {ok, _} -> false;
        {error, einval} ->
            case resolve(Host) of
                {ok, IPs} -> lists:member(IP, [A || {A, _} <- IPs]);
                {error, _} -> false
            end
    end;
host_match(Host, {cidr, _, _} = CIDR) ->
    case inet:parse_address(Host) of
        {ok, IP} ->
            cidr_match(IP, CIDR);
        {error, einval} ->
            case resolve(Host) of
                {ok, IPs} ->
                    lists:any(cidr_match(_, CIDR), [A || {A, _} <- IPs]);
                {error, _} -> false
            end
    end;
host_match(Host, {fqdn, Labels}) ->
    case misc:is_raw_ip(Host) of
        false ->
            HostLabels = string:split(string:lowercase(iolist_to_binary(Host)),
                                      ".", all),
            fqdn_match(HostLabels, Labels, true);
        true -> false
    end.

-ifdef(TEST).

host_match_test() ->
    ?assertEqual(true,  host_match("anything", any)),
    ?assertEqual(true,  host_match("10.10.1.44", any)),
    ?assertEqual(true,  host_match("127.0.0.1", {ip, {127,0,0,1}})),
    ?assertEqual(true,  host_match("::1", {ip, {0,0,0,0,0,0,0,1}})),
    ?assertEqual(true,  host_match("10.10.1.44",
                                   {cidr, {10, 10, 1, 32}, 27})),
    ?assertEqual(true,  host_match("exampLE.coM",
                                   {fqdn, [<<"example">>, <<"com">>]})),
    ?assertEqual(true,  host_match("3tesT-5.exampLE.coM",
                                   {fqdn, [<<"*">>, <<"example">>,
                                                <<"com">>]})),
    ?assertEqual(true,  host_match("u3tesT-554s.exampLE.coM",
                                   {fqdn, [<<"u3*54s">>, <<"example">>,
                                           <<"com">>]})),
    ?assertEqual(false, host_match("127.0.0.2", {ip, {127,0,0,1}})),
    ?assertEqual(false, host_match("127.0.0.a", {ip, {127,0,0,1}})),
    ?assertEqual(false, host_match("::1", {ip, {0,0,0,0,0,0,0,2}})),
    ?assertEqual(false, host_match("10.10.1.90",
                                   {cidr, {10, 10, 1, 32}, 27})),
    ?assertEqual(false, host_match("test.exampLE.coM",
                                   {fqdn, [<<"example">>, <<"com">>]})),
    ?assertEqual(false, host_match("test.exampL3.coM",
                                   {fqdn, [<<"test">>, <<"example">>,
                                           <<"com">>]})),
    ?assertEqual(false, host_match("3tesT-5.test.exampLE.coM",
                                   {fqdn, [<<"*">>, <<"*">>,
                                           <<"example">>, <<"com">>]})),
    ?assertEqual(false, host_match("u3tesT-564s.exampLE.coM",
                                   {fqdn, [<<"u3*54s">>, <<"example">>,
                                           <<"com">>]})),
    ?assertEqual(false, host_match("test1.test.example.com",
                                   {fqdn, [<<"*">>, <<"example">>,
                                           <<"com">>]})),
    ?assertEqual(false, host_match("test.foo.example.com",
                                   {fqdn, [<<"test">>, <<"*">>,
                                           <<"example">>, <<"com">>]})),
    ?assertEqual(false, host_match("*.example.com",
                                   {fqdn, [<<"*">>, <<"example">>,
                                           <<"com">>]})),
    ok.

cidr_match_test() ->
    Match = fun (IPStr, CIDRStr) ->
                {ok, IP} = inet:parse_address(IPStr),
                CIDR = menelaus_web_settings:parse_allowed_host(
                         list_to_binary(CIDRStr)),
                cidr_match(IP, CIDR)
            end,
    ?assertEqual(true,  Match("::", "::/0")),
    ?assertEqual(true,  Match("2001:4860::", "::/0")),
    ?assertEqual(false, Match("2001:4860::", "::/16")),
    ?assertEqual(false, Match("2001:4860::", "::/128")),

    ?assertEqual(true,  Match("::", "0234::/1")),
    ?assertEqual(true,  Match("7fff:ffff:ffff:ffff:ffff:ffff:ffff:ffff",
                              "0234::/1")),
    ?assertEqual(false, Match("8000::", "0234::/1")),
    ?assertEqual(false, Match("7fff:ffff:ffff:ffff:ffff:ffff:ffff:ffff",
                              "8201::/1")),
    ?assertEqual(true,  Match("8000::", "8201::/1")),
    ?assertEqual(true,  Match("ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff",
                             "8201::/1")),

    ?assertEqual(true,  Match("2001:4860::", "2001:4860::/32")),
    ?assertEqual(true,  Match("2001:4860:ffff:ffff:ffff:ffff:ffff:ffff",
                             "2001:4860::/32")),
    ?assertEqual(true,  Match("2001:4860:4860::8887", "2001:4860::/32")),
    ?assertEqual(false, Match("2001:4861:4860::8887", "2001:4860::/32")),
    ?assertEqual(false, Match("2001:4859:4860::8887", "2001:4860::/32")),

    ?assertEqual(true,  Match("2001:4860:ffff:ffff:ffff:ffff:ffff:1234",
                              "2001:4860:ffff:ffff:ffff:ffff:ffff:1234/128")),
    ?assertEqual(false, Match("2001:4860:ffff:ffff:ffff:ffff:ffff:1235",
                              "2001:4860:ffff:ffff:ffff:ffff:ffff:1234/128")),

    ?assertEqual(false, Match("2001:4860:ffff:ffff:ffff:ffff:ffff:fffd",
                             "2001:4860:ffff:ffff:ffff:ffff:ffff:fffe/127")),
    ?assertEqual(true,  Match("2001:4860:ffff:ffff:ffff:ffff:ffff:fffe",
                              "2001:4860:ffff:ffff:ffff:ffff:ffff:fffe/127")),
    ?assertEqual(true,  Match("2001:4860:ffff:ffff:ffff:ffff:ffff:ffff",
                              "2001:4860:ffff:ffff:ffff:ffff:ffff:fffe/127")),
    ?assertEqual(false, Match("2001:4861::",
                              "2001:4860:ffff:ffff:ffff:ffff:ffff:fffe/127")),

    ?assertEqual(true,  cidr_match({16#32, 16#ff, 16#ff, 16#ff},
                                   {cidr, {16#ff, 16#ff, 16#ff, 16#ff}, 0})),

    ?assertEqual(true,  cidr_match({16#ff, 16#ff, 16#ff, 16#ff},
                                   {cidr, {16#ff, 16#ff, 16#ff, 16#ff}, 32})),
    ?assertEqual(false, cidr_match({16#ff, 16#ff, 16#ff, 16#fe},
                                   {cidr, {16#ff, 16#ff, 16#ff, 16#ff}, 32})),


    ?assertEqual(false, cidr_match({2#01111111, 16#ff, 16#ff, 16#ff},
                                   {cidr, {16#8f, 16#ff, 16#ff, 16#ff}, 1})),
    ?assertEqual(true,  cidr_match({2#10000000, 16#00, 16#00, 16#00},
                                   {cidr, {16#8f, 16#ff, 16#ff, 16#ff}, 1})),
    ?assertEqual(true,  cidr_match({16#a4, 16#32, 16#43, 16#23},
                                   {cidr, {16#8f, 16#ff, 16#ff, 16#ff}, 1})),
    ?assertEqual(true,  cidr_match({16#ff, 16#ff, 16#ff, 16#ff},
                                   {cidr, {16#8f, 16#ff, 16#ff, 16#ff}, 1})),

    ?assertEqual(true,  cidr_match({16#00, 16#00, 16#00, 16#00},
                                   {cidr, {16#7f, 16#ff, 16#ff, 16#ff}, 1})),
    ?assertEqual(true,  cidr_match({16#7f, 16#ff, 16#ff, 16#ff},
                                   {cidr, {16#7f, 16#ff, 16#ff, 16#ff}, 1})),
    ?assertEqual(false, cidr_match({16#80, 16#00, 16#00, 16#00},
                                   {cidr, {16#7f, 16#ff, 16#ff, 16#ff}, 1})),


    ?assertEqual(true,  cidr_match({10, 10, 1, 44},
                                   {cidr, {10, 10, 1, 32}, 27})),
    ?assertEqual(false, cidr_match({10, 10, 1, 90},
                                   {cidr, {10, 10, 1, 32}, 27})),
    ?assertEqual(false, cidr_match({10, 10, 1, 31},
                                   {cidr, {10, 10, 1, 32}, 27})).

-endif.

fqdn_match([], [], _) -> true;
fqdn_match([], [_], _) -> false;
fqdn_match([_], [], _) -> false;
fqdn_match([E | T1], [E | T2], _) ->
    case E of
        <<"xn--", _/binary>> -> fqdn_match(T1, T2, false);
        _ ->
            case string:split(E, "*") of
                [_] -> fqdn_match(T1, T2, false);
                [_ | _] -> false
            end
    end;
fqdn_match([_E1 | _T1], [_E2 | _T2], false) ->
    false;
fqdn_match([E1 | T1], [E2 | T2], true) ->
    RE = iolist_to_binary(["^", string:replace(E2, "*", "[^.]*", all), "$"]),
    case re:run(E1, RE, [{capture, none}]) of
        match -> fqdn_match(T1, T2, false);
        nomatch -> false
    end.

cidr_match(IP, {cidr, CidrIP, CidrBits}) ->
    case size(IP) == size(CidrIP) of
        true ->
            DecimalIP = decimal_ip(IP),
            DecimalCidrIP = decimal_ip(CidrIP),
            Mask = cidr_mask(CidrIP, CidrBits),
            (DecimalIP band Mask) == (DecimalCidrIP band Mask);
        false -> false
    end.

%% IPv4
decimal_ip(IP) when size(IP) == 4 ->
    lists:foldl(fun (N, A) -> A bsl 8 + N end, 0, erlang:tuple_to_list(IP));
%% IPv6
decimal_ip(IP) when size(IP) == 8 ->
    lists:foldl(fun (N, A) -> A bsl 16 + N end, 0, erlang:tuple_to_list(IP)).

cidr_mask(IP, Bits) ->
    TotalBits = case size(IP) of
                    4 -> 32;
                    8 -> 128
                end,
    lists:foldl(fun (_, A) -> A bsl 1 + 1 end, 0, lists:seq(1, Bits))
        bsl (TotalBits - Bits).

-ifdef(TEST).
community_allowed_topologies_test() ->
    %% Test to help catch changes in community topologies that don't
    %% maintain backwards compatibility
    config_profile:mock_default_profile(),
    ?assertEqual(community_allowed_topologies(),
                 [[kv],[index,kv,n1ql],[fts,index,kv,n1ql]]),
    config_profile:unmock_default_profile(ok).
-endif.
