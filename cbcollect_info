#!/usr/bin/env python3
# -*- python -*-
#
# @author Couchbase <info@couchbase.com>
# @copyright 2011-Present Couchbase, Inc.
#
# Use of this software is governed by the Business Source License included in
# the file licenses/BSL-Couchbase.txt.  As of the Change Date specified in that
# file, in accordance with the Business Source License, use of this software
# will be governed by the Apache License, Version 2.0, included in the file
# licenses/APL2.txt.
import os
import sys
import tempfile
import time
import subprocess
import re
import platform
import glob
import socket
import threading
import optparse
import atexit
import signal
import urllib.parse
import shutil
import errno
import hashlib
import uuid
import configparser
from datetime import datetime, timedelta, tzinfo
import array
import mmap
from io import BytesIO, StringIO
from typing import BinaryIO, Dict, List, Optional, Iterable, Pattern, Tuple, TypeVar, Union, IO, Any
from zipfile import ZIP_DEFLATED, ZipFile, ZipInfo
from enum import Enum, unique
from io import BytesIO, StringIO
from abc import ABC, abstractmethod

""" Represents opaque 'data' from the C context """
_CData = TypeVar("_CData")

""" This is a copy of python's own type for allowed buffer type(s) """
ReadableBuffer = Union[bytes, bytearray, memoryview, array.array,
                       mmap.mmap, _CData]

""" A type representing one of the log processors """
LogProcessors = Union["RegularLogProcessor",
                      "AccessLogProcessor",
                      "CouchbaseLogProcessor"]

""" Type of allowed writers """
WriterType = Union[BytesIO, IO[bytes], "Writer"]

""" The default size of a single '.read()' operation """
READ_SIZE: int = 64 * 1024

""" The default log file """
DEFAULT_LOG: str = "couchbase.log"

""" Files that are included in redaction, but aren't ran through redactor """
BINARY_FILE_EXTS: Tuple[str, str] = (".gz", ".dmp")

""" Files in this list will be completely skipped in the redacted zip """
OMIT_IN_REDACT_ZIP: List[str] = ["users.dets"]

# This is a facility that provides a functionality similar to atexit from the
# standard library. We don't use the latter for the following reasons.
#
# When cbcollect_info is started with --watch-stdin flag, we start a thread
# monitoring stdin that terminates the process when stdin gets closed. The
# issue is many-fold:
#
#  - sys.exit() can only be called from the main thread.
#
#  - os._exit() doesn't invoke any of the cleanup functions registered by
#    atexit.
#
#  - It's possible for the stdin watcher thread to interrupt the main thread
#    by calling _thread.interrupt_main(). This plays nicely with atexit. But
#    the issue is that the thread can't always be interrupted. So it can take
#    a noticeable amount of time for the main thread to terminate.
#
# So AltExitC is a solution to these issues. It terminates the process as soon
# as possible by calling os._exit(). The price is that the cleanup actions
# need to be registered with AltExitC and synchronization is a concern.


class AltExitC(object):
    def __init__(self):
        self.list = []
        self.lock = threading.Lock()
        atexit.register(self.at_exit_handler)

    def register(self, f):
        self.lock.acquire()
        self.register_and_unlock(f)

    def register_and_unlock(self, f):
        try:
            self.list.append(f)
        finally:
            self.lock.release()

    def at_exit_handler(self):
        self.lock.acquire()
        self.list.reverse()
        for f in self.list:
            try:
                f()
            except BaseException:
                # Continue exit handling in spite of any exceptions
                pass

    def exit(self, status):
        self.at_exit_handler()
        os._exit(status)


AltExit = AltExitC()


class Writer(ABC):
    """
    Abstract base class for all of our writers. This is probably the "proper"
    method to use instead of overriding things that don't fit quite right
    for this use case. All our writers override this and combine in layers
    to provide a unified, streaming, interface.
    """

    def __init__(self) -> None:
        super().__init__()

    @abstractmethod
    def write(self, buf: ReadableBuffer) -> int:
        """
        This is the main method used by all the writers. It uses the same buffer
        type as most of the common python ones, for maximum interop between
        them.
        """
        pass

    @abstractmethod
    def flush(self):
        """"
        Some writers may maintain internal buffers, so this allows callers to
        flush out anything remaining in the buffer. This is especially useful
        if you are finished reading from the source and need to write the final
        bit that's still buffered.
        """
        pass

    def close(self):
        """
        Close is not required because many of the implementors will be passed
        in the underlying stream(s). If we were to close those inside the
        object and outside of it, that would be problematic/needless.
        """
        pass


class FSyncedFile(Writer):
    SYNC_BYTES = 16 * 1024 * 1024

    def __init__(self, *args, **kwargs):
        super().__init__()
        self._file = open(*args, **kwargs)
        self._written = 0

    def __getattr__(self, name):
        return getattr(self._file, name)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.close()

    def flush(self):
        """
        Choosing not to fsync here so we don't do it too often.
        """
        self._file.flush()

    def close(self):
        self._sync()
        self._file.close()

    def write(self, buf: ReadableBuffer):
        n = self._file.write(buf)

        self._written += n
        if self._written >= self.SYNC_BYTES:
            self._sync()
            self._written = 0

        return n

    def _sync(self):
        self._file.flush()
        os.fsync(self._file.fileno())


# Currently we decode bytes in this file via LATIN1. The reason for this is that
# UTF8 (which is the default in python) is a decoding which can fail - i.e. not
# all sequences of bytes are valid UTF8 and we cannot currenlty guarantee that
# all bytes that will be run through cbcollect will be valid UTF8. (We need
# protections elsewhere to make this guarantee that currently don't exist.) By
# contrast, all byte sequences are valid LATIN1, almost all our content is ASCII
# and thus LATIN1, and python2 essentially decoded strings as LATIN1, thus we
# are backwards compatible with pre-6.5 behavior. See MB-33809.
# For cases in which one knows for certain UTF8 is being used, feel free
# to use it.
LATIN1 = 'latin1'

USAGE = """usage: %prog [options] output_file.zip

- Linux/Windows/OSX:
    %prog output_file.zip
    %prog -v output_file.zip"""

# adapted from pytz


class LocalTZ(tzinfo):
    def __init__(self):
        offset = time.localtime().tm_gmtoff
        self._offset = timedelta(seconds=offset)

    def utcoffset(self, dt):
        return self._offset

    def dst(self, dt):
        return timedelta(0)

    def tzname(self, dt):
        return None


local_tz = LocalTZ()
log_stream = StringIO()
local_addr: Optional[str] = None
local_url_addr: Optional[str] = None


def set_local_addr(ipv6):
    global local_addr
    global local_url_addr

    local_addr = "::1" if ipv6 else "127.0.0.1"
    local_url_addr = "[::1]" if ipv6 else "127.0.0.1"


log_line: Optional[str] = None


def buffer_log_line(message, new_line):
    global log_line

    line = log_line
    if line is None:
        now = datetime.now(tz=local_tz)
        line = '[%s] ' % now.isoformat()

    line += message
    if new_line:
        log_line = None
        return line
    else:
        log_line = line
        return None


# Note: QE's collectinfo_test looks for "ERROR" or "Error" in the
# log messages and if found triggers a fatal error.
def log(message, new_line=True):
    global log_stream

    if new_line:
        message += '\n'

    bufline = buffer_log_line(message, new_line)
    if bufline is not None:
        log_stream.write(bufline)

    sys.stderr.write(message)
    sys.stderr.flush()


def generate_hash(val):
    return hashlib.sha1(val.encode())


class AccessLogProcessor:
    salt: str
    column_parser: Pattern[str]
    urls_to_redact: List[List[Any]]

    def __init__(self, salt):
        self.salt = salt
        self.column_parser = re.compile(
            r'(^\S* \S* )(\S*)( \[.*\] \"\S* )(\S*)( .*$)')
        self.urls_to_redact = [['/settings/rbac/users',
                                re.compile(r'\/(?P<user>[^\/\s#&]+)([#&]|$)'),
                                self._process_user, "user"],
                               ['/settings/rbac/lookupLDAPUser',
                                re.compile(r'\/(?P<user>[^\s#&]+)'),
                                self._process_user, "user"],
                               ['/_cbauth/checkPermission',
                                re.compile(r'user=(?P<user>[^\s&#]+)'),
                                self._process_user, "user"],
                               ['/pools/default/buckets',
                                re.compile(r'\/(?:[^\/\s#&]+)\/docs\/'
                                           '(?P<docid>[^\\/\\s#&]+)$'),
                                self._process_docid, "docid"]]

    def _process_url(self, surl):
        for conf in self.urls_to_redact:
            prefix = conf[0]
            if surl[:len(prefix)] == prefix:
                return prefix + self._process_url_tail(conf[1], conf[2],
                                                       conf[3],
                                                       surl[len(prefix):])
        return surl

    def _process_url_tail(self, rex, fn, key, s):
        m = rex.search(s)
        if m is not None:
            return s[:m.start(key)] + fn(m.group(key)) + s[m.end(key):]
        else:
            return s

    def _process_user(self, user):
        if user == '-' or user[0] == '@':
            return user
        elif user[-3:] == "/UI":
            return self._hash(user[:-3]) + "/UI"
        else:
            return self._hash(user)

    def _process_docid(self, docid):
        return self._hash(docid)

    def _hash(self, token):
        return generate_hash(self.salt + token).hexdigest()

    def _repl_columns(self, matchobj):
        return matchobj.group(1) + \
            self._process_user(matchobj.group(2)) + \
            matchobj.group(3) + \
            self._process_url(matchobj.group(4)) + \
            matchobj.group(5)

    def do(self, line):
        return self.column_parser.sub(self._repl_columns, line)


class RegularLogProcessor:
    salt: str
    rexes: List[Pattern[str]] = [
        re.compile("(<ud>)(.+?)(</ud>)"),
        # Redact the rest of the line in the case we encounter
        # log-redaction-salt. Needed to redact pre-6.5 debug logs
        # as well as occurence in couchbase.log
        re.compile("(log-redaction-salt)(.+)")]

    def __init__(self, salt):
        self.salt = salt

    def _hash(self, match):
        result = match.group(1)
        if match.lastindex == 3:
            h = generate_hash(self.salt + match.group(2)).hexdigest()
            result += h + match.group(3)
        elif match.lastindex == 2:
            result += " <redacted>"
        return result

    def _process_line(self, line):
        for rex in self.rexes:
            line = rex.sub(self._hash, line)
        return line

    def do(self, line):
        return self._process_line(line)


class CouchbaseLogProcessor(RegularLogProcessor):
    def do(self, line):
        if "RedactLevel" in line:
            # salt + salt to maintain consistency with other
            # occurances of hashed salt in the logs.
            return "RedactLevel:partial,HashOfSalt:" \
                f"{generate_hash(self.salt + self.salt).hexdigest()}\n"
        else:
            return self._process_line(line)


class ZipStream:
    """
    This is a wrapper around a normal ZipFile that offers an interface into it
    that lets us systematically write to individual files, in a specific way.

    Specifically, we must make sure to properly fill in the 'ZipInfo' with a
    compress_type, despite setting it on the main ZipFile upon creation.
    Otherwise the files will be written without compression.

    Beneath this, the zipfile is given a stream into the underlying file
    through the FSyncFile, so that we periodically sync to disk and don't just
    do one large fsync at the end.
    """
    _fp: FSyncedFile
    _zipfile: ZipFile
    _prefix: Optional[str]

    def __init__(self, zipfile: str, prefix: Optional[str]):
        self._fp = FSyncedFile(zipfile, "wb")
        self._zipfile = ZipFile(self._fp, mode="w", compression=ZIP_DEFLATED)
        self._prefix = prefix

    def open(self, path: str) -> "ZippedFileStream":
        """
        This will open a specific file in a ZipFile and return a
        ZippedFileStream which can be written into.
        """
        fullfilename = path
        if self._prefix:
            fullfilename = f"{self._prefix}/{path}"

        zinfo = ZipInfo(fullfilename, date_time=self.get_time_tuple_now())
        zinfo.compress_type = ZIP_DEFLATED
        zfile: IO[bytes] = self._zipfile.open(zinfo, mode="w")
        return ZippedFileStream(path, zfile)

    def close(self):
        self._fp.flush()
        self._zipfile.close()
        self._fp.close()

    @staticmethod
    def get_time_tuple_now():
        now = datetime.now()
        return (now.year, now.month, now.day, now.hour, now.minute, now.second)

    def __enter__(self):
        return self

    def __exit__(self, type, value, traceback):
        self.close()


class ZippedFileStream(Writer):
    """
    Individual file inside of a zipfile as a buffered stream object.
    """
    _filename: str
    _inner: IO[bytes]

    def __init__(self, filename: str, inner: IO[bytes]):
        super().__init__()
        self._filename = filename
        self._inner = inner

    def write(self, b: ReadableBuffer) -> int:
        """
        Overriden write function from Writer.
        """
        return write_readable_buffer(b, self._inner)

    def close(self):
        self.flush()
        return self._inner.close()

    def flush(self):
        return self._inner.flush()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, exc_traceback):
        self.close()


def write_readable_buffer(b: ReadableBuffer, writer: WriterType) -> int:
    return writer.write(convert_to_bytes(b))


def convert_to_bytes(b: ReadableBuffer) -> bytes:
    """
    The only reason for this nasty 'if isinstance' chain is that technically
    this interface takes a number of different types (as defined by the
    ReadableBuffer type) and some require slight conversions before being
    passed as bytes to the inner write which only takes bytes as input.
    """
    if isinstance(b, (bytes, bytearray)):
        return b
    if isinstance(b, mmap.mmap):
        return b.read()
    if isinstance(b, (array.array, memoryview)):
        return b.tobytes()
    else:
        raise Exception("Cannot support this input buffer type")


class RedactStream(Writer):
    _inner: WriterType
    _salt_value: str
    _filename: str
    _wraparound: List[str] = []

    """
    The redaction stream - redacts lines of text that pass through it.
    """

    def __init__(self, inner: WriterType, salt_value: str, filename: str):
        super().__init__()
        self._inner = inner
        self._salt_value = salt_value
        self._filename = filename
        self._wraparound = []

    def write(self, buf: ReadableBuffer) -> int:
        """
        Overridden write function of Writer. Decide whether or not to apply the
        redaction. Certain filetypes are categorically skipped (.gz, .dmp).
        """
        # skip binary files (don't redact)
        if self._filename.endswith(BINARY_FILE_EXTS):
            return self.write_passthrough(buf)

        # normal redaction path
        return self.write_redacted(buf)

    def write_redacted(self, buf: ReadableBuffer):
        """
        Redact, and then write, the buffer. This is slightly more complex than
        other parts of the streaming pipeline.

        This function takes the buffer, converts it all to a string, and then
        splits it into lines. If there are chunks at the end, that aren't
        terminated by a newline, we hold onto it until we receive more data
        that does.

        This list of non-terminated strings will accumulate until we get a
        newline, and we can combine all the chunks into a logical line and
        pass that to the redactor, which requires the data to be in lines
        and not just random chunks of text.
        """

        chunk = convert_to_bytes(buf).decode(LATIN1)
        lines = chunk.splitlines(keepends=True)
        last = None
        if not lines[-1].endswith(os.linesep):
            last = lines.pop()

        written: int = 0
        for line in lines:
            # if we have a portion leftover, write that first, and then clear
            # it so we don't write it again
            if self._wraparound:
                combined = f"{''.join(self._wraparound)}{line}"
                assert(combined.endswith(os.linesep))
                written = written + \
                    write_readable_buffer(self.process_line(combined),
                                          self._inner)
                self._wraparound.clear()
            else:
                # write the normal line
                written = written + \
                    write_readable_buffer(self.process_line(line), self._inner)
        if last:
            # Append any remaining string to our temporary list of non null
            # terminated strings that will eventually be combined as a full
            # line to be processed. In practice it doesn't seem like we
            # generally end up with more than one chunk in the queue before
            # flushing to redactors.
            self._wraparound.append(last)

        return written

    def write_passthrough(self, chunk: ReadableBuffer):
        return write_readable_buffer(chunk, self._inner)

    def flush(self):
        """
        Make sure we flush the remaining wrap-around data. This is especially
        important when we finish with a file, but it didn't also end with a
        newline.
        """
        if self._wraparound:
            self._inner.write(''.join(self._wraparound).encode(LATIN1))
            self._wraparound.clear()

        return self._inner.flush()

    def process_line(self, line: str) -> bytes:
        return self._redact_line(self._pick_redactor(self._filename),
                                 line).encode(LATIN1)

    def _pick_redactor(self, name: str):
        if "http_access" in name:
            return AccessLogProcessor(self._salt_value)
        elif name == DEFAULT_LOG:
            return CouchbaseLogProcessor(self._salt_value)
        else:
            return RegularLogProcessor(self._salt_value)

    def _redact_line(self, redactor: LogProcessors, line: str) -> str:
        return redactor.do(line)


class DoubleStream(Writer):
    _first: Writer
    _second: Writer

    def __init__(self, first: Writer, second: Writer):
        super().__init__()
        self._first = first
        self._second = second

    def write(self, buf: ReadableBuffer) -> int:
        first = self._first.write(buf)
        second = self._second.write(buf)
        self.flush()
        return first or second or 0

    def flush(self):
        self._first.flush()
        self._second.flush()


@unique
class Platform(Enum):
    SUNOS5 = "sunos5"
    SOLARIS = "solaris"
    LINUX = "linux"
    WIN32 = "win32"
    CYGWIN = "cygwin"
    DARWIN = "darwin"


class Task:
    platforms: List[Platform] = []
    output_file: str
    description: str = ""
    command: Union[str, List[str]] = ""
    timeout: Optional[int] = None
    use_shell: bool
    artifacts: Optional[List[str]] = None
    num_samples: int = 1
    interval: int = 0
    suppress_append_newline: bool = False
    to_stdin: Optional[str] = None
    no_header: bool = False
    change_dir: Union[bool, str] = False
    addenv: Optional[Iterable[Tuple[str, str]]] = None
    privileged: bool = False
    is_posix: bool = (os.name == "posix")
    extra_flags: Dict[str, str] = {}
    zip_relative_path: Optional[str] = None
    _task_runner: Optional["TaskRunner"] = None

    def __init__(self, description, command, timeout=None,
                 log_file=DEFAULT_LOG, artifacts=None, num_samples=1,
                 interval=0, suppress_append_newline=False, to_stdin=None,
                 no_header=False, change_dir=False, addenv=None,
                 privileged=False):
        self.output_file = log_file
        self.description = description
        self.command = command
        self.timeout = timeout
        self.use_shell = not isinstance(self.command, list)
        self.artifacts = artifacts
        self.num_samples = num_samples
        self.interval = interval
        self.suppress_append_newline = suppress_append_newline
        self.to_stdin = to_stdin
        self.no_header = no_header
        self.change_dir = change_dir
        self.addenv = addenv
        self.privileged = privileged
        self.is_posix = (os.name == "posix")

    def execute(self, outstream: Writer):
        log(f"{self.description} ({self.command}) - ", new_line=False)
        if not self.no_header:
            self.header(outstream, self)
        bad_result = None
        for i in range(self.num_samples):
            if i > 0:
                log(f"Taking sample {i + 1} after {self.interval}"
                    " seconds - ", new_line=False)
                time.sleep(self.interval)

            res = self.on_execute(outstream)
            if res != 0:
                bad_result = res
        if bad_result:
            return bad_result
        return 0

    def on_execute(self, outstream: Writer):
        p = None
        extra_flags = self._extra_flags()
        try:
            p = subprocess.Popen(self.command, bufsize=-1,
                                 stdin=subprocess.PIPE,
                                 stdout=subprocess.PIPE,
                                 stderr=subprocess.STDOUT,
                                 shell=self.use_shell,
                                 **extra_flags)
            if self.to_stdin and p.stdin:
                p.stdin.write(self.to_stdin.encode())
            if p.stdin:
                p.stdin.close()
        except OSError as e:
            # if use_shell is False then Popen may raise exception
            # if binary is missing. In this case we mimic what
            # shell does. Namely, complaining to stderr and
            # setting non-zero status code. It's might also
            # automatically handle things like "failed to fork due
            # to some system limit".
            outstream.write(
                f"Failed to execute {self.command}: {e}\n".encode())
            return 127
        except IOError as e:
            if e.errno == errno.EPIPE:
                outstream.write(
                    f"Ignoring broken pipe on stdin for {self.command}\n".encode())
            else:
                raise

        if p:
            stdout = p.stdout
        else:
            raise Exception("No stdout attached to process")

        from threading import Timer, Event

        timer = None
        timer_fired: Event = Event()

        if self.timeout is not None and self.can_kill(p):
            def on_timeout():
                try:
                    self._kill(p)
                except BaseException:
                    # the process might have died already
                    pass

                timer_fired.set()
            timer = Timer(self.timeout, on_timeout)
            timer.start()

        try:
            last_char_written = None
            while True and stdout is not None:
                data: bytes = stdout.read(READ_SIZE)
                if not data:
                    break
                outstream.write(data)
                last_char_written = data[-1:]
            self.maybe_append_newline(outstream, last_char_written)
        finally:
            if timer is not None:
                timer.cancel()
                timer.join()
                # there's a tiny chance that command succeeds just before
                # timer is fired; that would result in a spurious timeout
                # message
                if timer_fired.is_set():
                    outstream.write(f"`{self.command}` timed out "
                                    f"after {self.timeout} seconds\n".encode())
                    log(f"[Command timed out after {self.timeout} seconds] - ",
                        new_line=False)
            if stdout:
                stdout.close()

        code = 0
        if p:
            code = p.wait()

        if stdout:
            stdout.close()

        outstream.flush()
        return code

    def will_run(self):
        """Determine if this task will run on this platform."""
        return Platform[sys.platform.upper()] in self.platforms

    def satisfies_preconditions(self) -> bool:
        if self.privileged and os.getuid() != 0:
            log("skipped (needs root privs)")
            return False
        return True

    def maybe_append_newline(self, fp, last_char: Optional[bytes]):
        """Append a newline (if appropriate) to ensure that the next
        header starts on a new line.
        """
        if self.suppress_append_newline:
            return
        if self.no_header:
            # The "no_header" attribute indicates that this task
            # produces a single result which does not contain a header.
            # Thus, we shouldn't append a new line to the result.
            return
        if last_char and last_char.decode() != os.linesep:
            fp.write(os.linesep.encode(LATIN1))

    @staticmethod
    def log_result(result):
        if result == 0 or result is None:
            log("OK")
        else:
            log(f"Exit code {result}")

    @staticmethod
    def header(fp: Writer, task: "Task"):
        separator = "=" * 78
        subtitle = task.command
        if isinstance(task.command, list):
            subtitle = " ".join(task.command)
        message = f"{separator}\n{task.description}\n{subtitle}\n{separator}\n"
        fp.write(message.encode())

    def can_kill(self, p: subprocess.Popen):
        if self.is_posix:
            return True
        return hasattr(p, "kill")

    def set_task_runner(self, runner: "TaskRunner"):
        self._task_runner = runner

    def _kill(self, p: subprocess.Popen):
        if self.is_posix:
            group_pid = os.getpgid(p.pid)
            os.killpg(group_pid, signal.SIGKILL)
        else:
            p.kill()

    def _extra_flags(self) -> Dict[str, Any]:
        flags = self._env_flags()
        flags.update(self._platform_popen_flags())
        flags.update(self._cwd_flags())
        return flags

    def _cwd_flags(self) -> Dict[str, str]:
        flags = {}
        if self.change_dir and self._task_runner:
            cwd = self._task_runner.tmpdir
            if isinstance(self.change_dir, str):
                cwd = self.change_dir
            flags["cwd"] = cwd
        return flags

    def _platform_popen_flags(self) -> Dict[str, Any]:
        flags = {}
        if self.is_posix:
            flags["preexec_fn"] = os.setpgrp
        return flags

    def _env_flags(self) -> Dict[str, Any]:
        flags = {}
        if self.addenv:
            env = os.environ.copy()
            env.update(self.addenv)
            flags["env"] = env
        return flags

    def __repr__(self):
        return f"<{self.__class__.__qualname__}: {self.__dict__}>"

    def __str__(self):
        return f"<{self.__class__.__qualname__}: {self.__dict__}>"


class SolarisTask(Task):
    platforms = [Platform.SUNOS5, Platform.SOLARIS]


class LinuxTask(Task):
    platforms = [Platform.LINUX]


class WindowsTask(Task):
    platforms = [Platform.WIN32, Platform.CYGWIN]


class MacOSXTask(Task):
    platforms = [Platform.DARWIN]


class UnixTask(SolarisTask, LinuxTask, MacOSXTask):
    platforms = SolarisTask.platforms + LinuxTask.platforms \
        + MacOSXTask.platforms


class AllOsTask(UnixTask, WindowsTask):
    platforms = UnixTask.platforms + WindowsTask.platforms


class LiteralTask(AllOsTask):
    literal: str

    def __init__(self, description, literal, timeout=None,
                 log_file=DEFAULT_LOG, no_header=False):
        self.description = description
        self.literal = literal
        self.timeout = timeout
        self.output_file = log_file
        self.no_header = no_header

    def on_execute(self, outstream: Writer):
        literal = f"{self.literal}\n"
        outstream.write(literal.encode())
        outstream.flush()
        return 0


class CollectFileTask(AllOsTask):
    def __init__(self, description, file_path, zip_relative: str = ""):
        self.description = description
        self.output_file = file_path
        if zip_relative == "":
            zip_relative = os.path.basename(file_path)
        self.zip_relative_path = zip_relative
        self.no_header = True

    def on_execute(self, outstream: Writer):
        try:
            with open(self.output_file, "rb") as f:
                while True:
                    res = f.read(READ_SIZE)
                    if not res:
                        break
                    outstream.write(res)
                outstream.flush()
                return 0
        except FileNotFoundError:
            log(f"File doesn't exist: {self.output_file} -- ", new_line=False)
            return 1

    @staticmethod
    def create_directory_collection_tasks(dir_path, relative_path_base):
        tasks = []
        for dirpath, _, filenames in os.walk(dir_path):
            for f in filenames:
                full_path = os.path.join(dirpath, f)
                relative_path = os.path.relpath(full_path, dir_path)
                path_in_zip = os.path.join(relative_path_base, relative_path)
                task = CollectFileTask(f"Collecting {full_path}",
                                       full_path,
                                       zip_relative=path_in_zip)
                tasks.append(task)
        return tasks

    def satisfies_preconditions(self) -> bool:
        return os.path.exists(self.output_file)


class MetaTask:
    subtasks: List["Task"] = []
    tmp_dir: Optional[str] = None
    pending_artifacts: List["MetaTask"] = []
    output_file: str

    def __init__(self, *subtasks: Task, filename=DEFAULT_LOG,
                 tmp_dir=None):
        self.output_file = filename
        self.subtasks = list(subtasks)
        self.tmp_dir = tmp_dir
        self.pending_artifacts = []

    def execute_all(self, outstream: Writer):
        for task in self.subtasks:
            if hasattr(self, "_task_runner") and self._task_runner:
                task.set_task_runner(self._task_runner)

            if task.satisfies_preconditions():
                Task.log_result(task.execute(outstream))

            # Handle artifacts created during that task
            # Note: We cannot just add more items to the current set of
            # iterating metatasks but instead must just run this directly
            # after.
            self.add_pending_artifacts(task)

    def add_pending_artifacts(self, task: Task):
        if task.artifacts:
            for artifact in task.artifacts:
                path = artifact
                if not os.path.isabs(path) and self.tmp_dir:
                    # We assume that "relative" artifacts are produced
                    # in the self.tmpdir
                    path = os.path.join(self.tmp_dir, artifact)
                relative_name = os.path.basename(path)
                t = MetaTask(CollectFileTask(f"Collect {artifact}", path,
                                             zip_relative=relative_name),
                             filename=relative_name, tmp_dir=self.tmp_dir)
                t.set_task_runner(self._task_runner)
                self.pending_artifacts.append(t)

    def append(self, item: Task):
        self.subtasks.append(item)

    def set_task_runner(self, runner: Optional["TaskRunner"]):
        self._task_runner = runner

    def get_artifacts(self):
        return self.pending_artifacts

    def satisfies_preconditions(self) -> bool:
        for t in self.subtasks:
            # If any of the preconditions are valid in this metatask, run it.
            if t.satisfies_preconditions():
                return True
        return False


class TaskRunner:
    task_regexp: Pattern
    tmpdir: str
    zip_out: ZipStream
    redacted_zip_out: Optional[ZipStream] = None
    zip_name: str
    zip_name_redacted: Optional[str] = None

    def __init__(self, zipfile, verbosity=0, task_regexp="", tmp_dir=None,
                 salt_value="", prefix=None):
        self.verbosity = verbosity
        self.start_time = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
        self.salt_value = salt_value

        # Depending on platform, mkdtemp() may act unpredictably if passed an
        # empty string.
        if not tmp_dir:
            tmp_dir = None
        else:
            tmp_dir = os.path.abspath(os.path.expanduser(tmp_dir))

        try:
            self.tmpdir = tempfile.mkdtemp(dir=tmp_dir)
        except OSError as e:
            log("Could not use temporary dir {0}: {1}".format(tmp_dir, e))
            sys.exit(1)

        # If a dir wasn't passed by --tmp-dir, check if the env var was set and
        # if we were able to use it
        if not tmp_dir and os.getenv("TMPDIR") and os.path.split(
                self.tmpdir)[0] != os.getenv("TMPDIR"):
            log("Could not use TMPDIR {0}".format(os.getenv("TMPDIR")))
        log("Using temporary dir {0}".format(os.path.split(self.tmpdir)[0]))

        self.task_regexp = re.compile(task_regexp)
        AltExit.register(self.finalize)

        self.zip_name = zipfile
        self.zip_out = ZipStream(self.zip_name, prefix)
        redact_zip_file: str = zipfile[:-4] + "-redacted" + zipfile[-4:]
        self.zip_name_redacted = redact_zip_file
        if self.salt_value:
            self.redacted_zip_out = ZipStream(self.zip_name_redacted, prefix)

    def run_tasks(self, *tasks: Task):
        categorized = self._categorize_tasks(*tasks, tmp_dir=self.tmpdir)
        for _, metatask in categorized.items():
            metatask.set_task_runner(self)
            self.run(metatask)

    def run(self, task: MetaTask):
        self._run_metatask(task)

    def literal_task(self, description, value, **kwargs):
        return LiteralTask(description, value, **kwargs)

    def close(self):
        """
        Closes the zipfile(s) used. This is only needed in tests if you need to
        use the zipfiles before the taskrunner is dropped / finalized.
        """
        self.zip_out.close()
        if self.redacted_zip_out:
            self.redacted_zip_out.close()

    def finalize(self):
        self.close()
        shutil.rmtree(self.tmpdir, ignore_errors=True)

    def _categorize_tasks(self, *tasks: Task, tmp_dir=None):
        output: Dict[str, MetaTask] = {}
        for task in tasks:
            if self.task_regexp and self.task_regexp.match(
                    task.description) is None:
                log(f"Skipping task {task.description} because "
                    f"it doesn't match '{self.task_regexp.pattern}'")
                continue
            if not task.will_run():
                continue
            task.set_task_runner(self)
            entry = output.get(task.output_file)
            if entry:
                entry.append(task)
            else:
                if task.zip_relative_path:
                    outfile = task.zip_relative_path
                else:
                    outfile = task.output_file
                output[task.output_file] = MetaTask(task, filename=outfile,
                                                    tmp_dir=tmp_dir)
        return output

    def _run_metatask(self, metatask: MetaTask):
        # If the required precondition(s) fail, skip the task. Right now this
        # is only really used to make sure we don't create empty files for
        # CollectFileTask's that don't point to real files.
        if not metatask.satisfies_preconditions():
            return

        if self.redacted_zip_out:
            if metatask.output_file not in OMIT_IN_REDACT_ZIP:
                self._write_both(metatask)
            else:
                self._write_unredacted(metatask)
        else:
            self._write_unredacted(metatask)

        # Handle artifacts created by that metatask
        for artifact in metatask.get_artifacts():
            self._run_metatask(artifact)

    def _write_both(self, metatask: MetaTask):
        if self.redacted_zip_out:
            with self.redacted_zip_out.open(metatask.output_file) as rlogfile:
                redact = RedactStream(rlogfile, self.salt_value,
                                      metatask.output_file)
                with self.zip_out.open(metatask.output_file) as logfile:
                    double = DoubleStream(redact, logfile)
                    metatask.execute_all(double)

    def _write_unredacted(self, metatask: MetaTask):
        with self.zip_out.open(metatask.output_file) as logfile:
            metatask.execute_all(logfile)


def make_curl_task(name, user, password, url,
                   timeout=60, log_file=DEFAULT_LOG, base_task=AllOsTask,
                   **kwargs):
    return base_task(name, ["curl", "-sS", "-k", "--proxy", "", "-K-", url],
                     timeout=timeout,
                     log_file=log_file,
                     to_stdin="--user %s:%s" % (user, password),
                     **kwargs)


def make_curl_post_task(name, user, password, url, timeout=60,
                        log_file=DEFAULT_LOG, post_data="",
                        base_task=AllOsTask, **kwargs):
    return base_task(name, ["curl", "-sS", "-X", "POST", "--proxy", "", "-K-",
                     url],
                     timeout=timeout,
                     log_file=log_file,
                     to_stdin=f"--user {user}:{password}\n --data {post_data}",
                     **kwargs)


def make_cbstats_task(kind, memcached_pass, guts, extra_args=""):
    port = read_guts(guts, "memcached_dedicated_port")
    user = read_guts(guts, "memcached_admin")
    return AllOsTask(f"memcached stats {kind}",
                     f"cbstats {local_url_addr}:{port} -u {user} {kind} "
                     f"{extra_args}",
                     log_file="stats.log",
                     timeout=60,
                     addenv=[("CB_PASSWORD", memcached_pass)])


def make_cbstats_all_buckets_task(kind, memcached_pass, guts):
    # The extra arg "-a" tells cbstats to iterate over all Buckets
    return make_cbstats_task(kind, memcached_pass, guts, "-a")


def get_local_token(guts):
    path = read_guts(guts, "localtoken_path")
    token = ""
    try:
        with open(path, 'r') as f:
            token = f.read().rstrip('\n')
    except IOError as e:
        log("I/O error(%s): %s" % (e.errno, e.strerror))
    return token


def get_diag_password(guts):
    port = read_guts(guts, "rest_port")
    pwd = get_local_token(guts)
    url = "http://%s:%s/diag/password" % (local_url_addr, port)
    command = ["curl", "-sS", "--proxy", "", "-u", "@localtoken:%s" % pwd, url]

    # Don't append a newline to the result, since that would corrupt
    # the password.
    task = AllOsTask("get diag password", command, timeout=60, no_header=True,
                     suppress_append_newline=True)
    output_bytes = BytesIO()
    status = task.execute(output_bytes)
    output = output_bytes.getvalue().decode(LATIN1)
    if status == 0:
        return output
    log(output)
    return ""


def make_query_statement_task(statement, user, password, port,
                              logfile=DEFAULT_LOG, **kwargs):
    url = "http://%s:%s/query/service?statement=%s" % (
        local_url_addr, port, urllib.parse.quote(statement))

    return make_curl_task(name="Result of query statement \'%s\'" % statement,
                          user=user, password=password, url=url,
                          log_file=logfile, **kwargs)


def make_cbas_statement_task(statement, user, password, port):
    url = "http://%s:%s/analytics/service/diagnostics?statement=%s" % (
        local_url_addr, port, urllib.parse.quote(statement))

    return make_curl_task(name="Result of query statement \'%s\'" % statement,
                          user=user, password=password, url=url)


def make_index_task(
        name,
        api,
        passwd,
        index_port,
        logfile=DEFAULT_LOG,
        **kwargs):
    index_url = f'http://{local_url_addr}:{index_port}/{api}'
    return make_curl_task(
        name,
        "@",
        passwd,
        index_url,
        log_file=logfile,
        **kwargs)


def make_golang_profile_tasks(service_name, passwd, port, log_prefix,
                              no_header=True, tls=False, **kwargs):
    """
    Helper function to create the various tasks needed to collect profiling
    information from Golang components
    :param service_name:    The service name, for example 'Query'
    :param passwd:          The password used to auth against the API
    :param port:            The port used to connect to the API on
    :param log_prefix:      String to append to the start of each log file
    :param no_header:       Whether to append the usual command header to the
                            log file, defaults to True
    :returns:               A list of tasks to run
    """

    # A list of tuples containing the APIs that we are going to hit.
    # The tuple is in the format (Description, Log Postfix, API URL).
    # If new profiling is ever needed, can add a new item to this list.
    apis = [('Go Routine Dump', '_pprof', 'debug/pprof/goroutine?debug=1'),
            ('Go Routine Dump2', '_pprof2', 'debug/pprof/goroutine?debug=2'),
            ('CPU Profile', '_cprof', 'debug/pprof/profile?seconds=30'),
            ('Memory Profile', '_mprof', 'debug/pprof/heap')]

    # Iterate through each API, create the full URL, and then append the
    # resulting cURL task to the list of tasks
    secure = "s" if tls else ""
    base_url = f'http{secure}://{local_url_addr}:{port}'

    tasks = []
    for descr, postfix, api in apis:
        if postfix == "_pprof2" and service_name != "Query":
            continue
        api_url = f'{base_url}/{api}'
        name = f'{service_name} {descr}: '
        logfile = f'{log_prefix}{postfix}.log'
        tasks.append(
            make_curl_task(
                name,
                '@',
                passwd,
                api_url,
                log_file=logfile,
                no_header=no_header,
                **kwargs))
    return tasks


def make_redaction_task():
    return LiteralTask("Log Redaction", "RedactLevel:none")


def make_chronicle_dump_task(name, args,
                             initargs_path, log_file=DEFAULT_LOG):
    escript = exec_name("escript")
    escript_wrapper = find_script("escript-wrapper")
    chronicle_dump_path = find_script("chronicle_dump")

    if escript_wrapper is None or chronicle_dump_path is None:
        return []

    return AllOsTask(name,
                     [escript,
                      escript_wrapper,
                      "--initargs-path", initargs_path, "--",
                      chronicle_dump_path] + args,
                     timeout=600,
                     log_file=log_file)


def make_chronicle_snapshots_task(guts, initargs_path):
    chronicle_snapshot_dir = read_guts(guts, "chronicle_snapshot_dir")
    if chronicle_snapshot_dir is None:
        return []

    snapshots = [os.path.join(chronicle_snapshot_dir, f.name)
                 for f in os.scandir(chronicle_snapshot_dir)
                 if f.is_file() and f.name.endswith('.snapshot')]

    return make_chronicle_dump_task(
        "Chronicle dump",
        ["snapshot",
         "--sanitize", "chronicle_kv_log:sanitize_snapshot"] + snapshots,
        initargs_path)


def make_chronicle_logs_task(guts, initargs_path):
    chronicle_dir = read_guts(guts, "chronicle_dir")
    if not chronicle_dir:
        return []

    pattern = os.path.join(chronicle_dir, "logs", "*.log")
    logs = glob.glob(pattern)

    def log_ix(path):
        ix, _ = os.path.splitext(os.path.basename(path))

        try:
            return int(ix)
        except ValueError:
            return -1

    logs.sort(key=log_ix)

    return make_chronicle_dump_task(
        "Chronicle logs",
        ["log", "--sanitize", "chronicle_kv_log:sanitize_log"] + logs,
        initargs_path,
        log_file="chronicle_logs.log")


def basedir():
    # We are installed in $INSTALL_DIR/lib/python, so need to go up three
    # levels
    return os.path.normpath(
        os.path.join(
            os.path.abspath(__file__),
            '..', '..', '..'
        )
    )


def make_event_log_task():
    from datetime import datetime, timedelta

    # I found that wmic ntevent can be extremely slow; so limiting the output
    # to approximately last month
    limit = datetime.today() - timedelta(days=31)
    limit = limit.strftime('%Y%m%d000000.000000-000')

    return WindowsTask(
        "Event log",
        "wmic ntevent where "
        "\""
        "(LogFile='application' or LogFile='system') and "
        "EventType<3 and TimeGenerated>'%(limit)s'"
        "\" "
        "get TimeGenerated,LogFile,SourceName,EventType,Message "
        "/FORMAT:list" %
        locals())


def make_os_tasks():
    programs = " ".join(["memcached", "beam.smp",
                         "couch_compact", "godu", "sigar_port",
                         "cbq-engine", "indexer", "projector", "goxdcr",
                         "cbft", "eventing-producer", "eventing-consumer"])

    _tasks = [
        UnixTask("uname", "uname -a"),
        UnixTask("time and TZ", "date; date -u"),
        UnixTask("ntp time",
                 "ntpdate -q pool.ntp.org || "
                 "nc time.nist.gov 13 || "
                 "netcat time.nist.gov 13", timeout=60),
        UnixTask("ntp peers", "ntpq -p"),
        UnixTask("raw /etc/sysconfig/clock", "cat /etc/sysconfig/clock"),
        UnixTask("raw /etc/timezone", "cat /etc/timezone"),
        LinuxTask("Available clock sources",
                  "cat /sys/devices/system/clocksource/clocksource0/available_clocksource"),
        LinuxTask("Current clock source",
                  "cat /sys/devices/system/clocksource/clocksource0/current_clocksource"),
        WindowsTask("System information", "systeminfo"),
        WindowsTask("Computer system", "wmic computersystem"),
        WindowsTask("Computer OS", "wmic os"),
        LinuxTask("System Hardware", "lshw -json || lshw"),
        SolarisTask("Process list snapshot",
                    "prstat -a -c -n 100 -t -v -L 1 10"),
        SolarisTask("Process list", "ps -ef"),
        SolarisTask("Service configuration", "svcs -a"),
        SolarisTask("Swap configuration", "swap -l"),
        SolarisTask("Disk activity", "zpool iostat 1 10"),
        SolarisTask("Disk activity", "iostat -E 1 10"),
        LinuxTask("Process list snapshot",
                  "export TERM=''; top -Hb -n1 || top -H n1"),
        LinuxTask(
            "Process list",
            "ps -AwwL -o user,pid,lwp,ppid,nlwp,pcpu,maj_flt,min_flt,pri,nice,vsize,rss,tty,stat,wchan:12,start,"
            "bsdtime,comm,command"),
        LinuxTask("Raw /proc/buddyinfo", "cat /proc/buddyinfo"),
        LinuxTask("Raw /proc/meminfo", "cat /proc/meminfo"),
        LinuxTask("Raw /proc/pagetypeinfo", "cat /proc/pagetypeinfo"),
        LinuxTask("Raw /proc/zoneinfo", "cat /proc/zoneinfo"),
        LinuxTask("Raw /proc/vmstat", "cat /proc/vmstat"),
        LinuxTask("Raw /proc/mounts", "cat /proc/mounts"),
        LinuxTask("Raw /proc/partitions", "cat /proc/partitions"),
        LinuxTask("Raw /proc/diskstats",
                  "cat /proc/diskstats; echo ''", num_samples=10, interval=1),
        LinuxTask("Raw /proc/interrupts", "cat /proc/interrupts"),
        LinuxTask("Swap configuration", "free -t"),
        LinuxTask("Swap configuration", "swapon -s"),
        LinuxTask("Kernel modules", "lsmod"),
        LinuxTask("Distro version", "cat /etc/redhat-release"),
        LinuxTask("Distro version", "cat /etc/oracle-release"),
        LinuxTask("Distro version", "cat /etc/debian_version"),
        LinuxTask("Distro version", "lsb_release -a"),
        LinuxTask("Distro version", "cat /etc/SuSE-release"),
        LinuxTask("Distro version", "cat /etc/issue"),
        LinuxTask("Distro version", "cat /etc/os-release"),
        LinuxTask("Distro version", "cat /etc/system-release"),
        LinuxTask("Installed software", "rpm -qa"),
        LinuxTask("Ksplice updates", "uptrack-show"),
        LinuxTask("Hot fix list", "rpm -V couchbase-server"),
        # NOTE: AFAIK columns _was_ necessary, but it doesn't appear to be
        # required anymore. I.e. dpkg -l correctly detects stdout as not a
        # tty and stops playing smart on formatting. Lets keep it for few
        # years and then drop, however.
        LinuxTask("Installed software", "COLUMNS=300 dpkg -l"),
        # NOTE: -V is supported only from dpkg v1.17.2 onwards.
        LinuxTask("Hot fix list", "COLUMNS=300 dpkg -V couchbase-server"),
        LinuxTask(
            "Extended iostat",
            "iostat -x -p ALL 1 10 || iostat -x 1 10"),
        LinuxTask("Core dump settings",
                  "find /proc/sys/kernel -type f -name '*core*' -print -exec cat '{}' ';'"),
        UnixTask("sysctl settings", "sysctl -a"),
        LinuxTask("Relevant lsof output",
                  "echo %(programs)s | xargs -n1 pgrep -f | xargs -n1 -r -- lsof -n -p" % locals()),
        LinuxTask("LVM info", "lvdisplay"),
        LinuxTask("LVM info", "vgdisplay"),
        LinuxTask("LVM info", "pvdisplay"),
        LinuxTask("Block device queue settings",
                  "find /sys/block/*/queue /sys/block/*/device/queue_* -type f | xargs grep -vH xxxx | sort"),
        MacOSXTask("Process list snapshot", "top -l 1"),
        MacOSXTask("Disk activity", "iostat 1 10"),
        MacOSXTask("Process list",
                   "ps -Aww -o user,pid,lwp,ppid,nlwp,pcpu,pri,nice,vsize,rss,tty,"
                   "stat,wchan:12,start,bsdtime,command"),
        WindowsTask("Installed software", "wmic product get name, version"),
        WindowsTask(
            "Service list", "wmic service where state=\"running\" GET caption, name, state"),
        WindowsTask("Process list", "wmic process"),
        WindowsTask("Process usage", "tasklist /V /fo list"),
        WindowsTask("Swap settings", "wmic pagefile"),
        WindowsTask("Disk partition", "wmic partition"),
        WindowsTask("Disk volumes", "wmic volume"),
        UnixTask("Network configuration", "ifconfig -a", interval=10,
                 num_samples=2),
        LinuxTask("Network configuration",
                  "echo link addr neigh rule route netns | xargs -n1 -- sh -x -c 'ip $1 list' --"),
        WindowsTask("Network configuration", "ipconfig /all", interval=10,
                    num_samples=2),
        LinuxTask("Raw /proc/net/dev", "cat /proc/net/dev"),
        LinuxTask("Network link statistics", "ip -s link"),
        UnixTask("Network status", "netstat -anp || netstat -an"),
        WindowsTask("Network status", "netstat -anotb"),
        AllOsTask("Network routing table", "netstat -rn"),
        LinuxTask("Network socket statistics", "ss -an"),
        LinuxTask("Extended socket statistics",
                  "ss -an --info --processes --memory --options",
                  timeout=300),
        UnixTask("Arp cache", "arp -na"),
        LinuxTask("Iptables dump", "iptables-save"),
        UnixTask("Raw /etc/hosts", "cat /etc/hosts"),
        UnixTask("Raw /etc/resolv.conf", "cat /etc/resolv.conf"),
        UnixTask("Raw /etc/nsswitch.conf", "cat /etc/nsswitch.conf"),
        WindowsTask("Arp cache", "arp -a"),
        WindowsTask("Network Interface Controller", "wmic nic"),
        WindowsTask("Network Adapter", "wmic nicconfig"),
        WindowsTask("Active network connection", "wmic netuse"),
        WindowsTask("Protocols", "wmic netprotocol"),
        WindowsTask(
            "Hosts file", "type %SystemRoot%\system32\drivers\etc\hosts"),
        WindowsTask("Cache memory", "wmic memcache"),
        WindowsTask("Physical memory", "wmic memphysical"),
        WindowsTask("Physical memory chip info", "wmic memorychip"),
        WindowsTask("Local storage devices", "wmic logicaldisk"),
        UnixTask("Filesystem", "df -ha"),
        UnixTask("Filesystem inodes", "df -i"),
        UnixTask("System activity reporter", "sar 1 10"),
        UnixTask("System paging activity", "vmstat 1 10"),
        UnixTask("System uptime", "uptime"),
        UnixTask("Last logins of users and ttys", "last -x || last"),
        UnixTask("couchbase user definition", "getent passwd couchbase"),
        UnixTask("couchbase user limits", "su couchbase -s /bin/sh -c \"ulimit -a\"",
                 privileged=True),
        UnixTask("Interrupt status", "intrstat 1 10"),
        UnixTask("Processor status", "mpstat 1 10"),
        UnixTask("System log", "cat /var/adm/messages"),
        LinuxTask("Raw /proc/uptime", "cat /proc/uptime"),
        LinuxTask("Systemd journal",
                  "journalctl | gzip -c > systemd_journal.gz",
                  change_dir=True, artifacts=['systemd_journal.gz'],
                  suppress_append_newline=True),
        LinuxTask("All logs",
                  "tar cz /var/log/syslog* /var/log/dmesg /var/log/messages* /var/log/daemon* /var/log/debug* "
                  "/var/log/kern.log* 2>/dev/null",
                  log_file="syslog.tar.gz", no_header=True),
        LinuxTask("Relevant proc data", "echo %(programs)s | "
                  "xargs -n1 pgrep -f | xargs -n1 -- sh -c 'echo $1; cat /proc/$1/status; cat /proc/$1/limits; "
                  "cat /proc/$1/task/*/sched; echo' --" % locals()),
        LinuxTask("Processes' environment", "echo %(programs)s | "
                  r"xargs -n1 pgrep -f | xargs -n1 -- sh -c 'echo $1; ( cat /proc/$1/environ | tr \\0 \\n | "
                  "egrep -v ^CB_MASTER_PASSWORD=\|^CBAUTH_REVRPC_URL=); echo' --" % locals()),
        LinuxTask("Processes' stack",
                  "for program in %(programs)s; do for thread in $(pgrep --lightweight $program); "
                  "do echo $program/$thread:; cat /proc/$thread/stack; echo; done; done" % locals()),
        LinuxTask("NUMA data", "numactl --hardware"),
        LinuxTask("NUMA data", "numactl --show"),
        LinuxTask("NUMA data", "cat /sys/devices/system/node/node*/numastat"),
        UnixTask("Kernel log buffer", "dmesg -T || dmesg -H || dmesg"),
        LinuxTask("Transparent Huge Pages data",
                  "cat /sys/kernel/mm/transparent_hugepage/enabled"),
        LinuxTask("Transparent Huge Pages data",
                  "cat /sys/kernel/mm/transparent_hugepage/defrag"),
        LinuxTask("Transparent Huge Pages data",
                  "cat /sys/kernel/mm/redhat_transparent_hugepage/enabled"),
        LinuxTask("Transparent Huge Pages data",
                  "cat /sys/kernel/mm/redhat_transparent_hugepage/defrag"),
        LinuxTask("Network statistics", "netstat -s"),
        LinuxTask("Full raw netstat", "cat /proc/net/netstat"),
        LinuxTask("CPU throttling info",
                  "echo /sys/devices/system/cpu/cpu*/thermal_throttle/* | xargs -n1 -- sh -c 'echo $1; cat $1' --"),
        LinuxTask("Raw PID 1 scheduler /proc/1/sched",
                  "cat /proc/1/sched | head -n 1"),
        LinuxTask("Raw PID 1 control groups /proc/1/cgroup",
                  "cat /proc/1/cgroup"),
        LinuxTask("SysFs Control Group data",
                  "find /sys/fs/cgroup -type f -print0 | sort --zero-terminated | xargs --null grep . -H"),
        make_event_log_task(),
    ]

    return _tasks

# stolen from
# http://rightfootin.blogspot.com/2006/09/more-on-python-flatten.html


def iter_flatten(iterable):
    it = iter(iterable)
    for e in it:
        if isinstance(e, (list, tuple)):
            for f in iter_flatten(e):
                yield f
        else:
            yield e


def flatten(iterable):
    return [e for e in iter_flatten(iterable)]


def read_guts(guts, key):
    return guts.get(key, "")


def populate_guts_with_additional_info(guts):
    add_db_idx_dirs(guts)
    add_required_data_paths(guts)
    add_bucket_info(guts)
    add_chronicle_snapshot_dir(guts)


def add_chronicle_snapshot_dir(guts):
    guts["chronicle_snapshot_dir"] = get_chronicle_snapshot_dir(guts)


def add_required_data_paths(guts):
    data_dir = guts.get("path_config_datadir")
    if data_dir is None:
        return
    guts["indexer_breakpad_minidump_dir"] = os.path.join(data_dir, "crash")
    guts["users_storage_path"] = os.path.join(data_dir, "config", "users.dets")
    guts["dist_cfg_path"] = os.path.join(data_dir, "config", "dist_cfg")
    guts["chronicle_dir"] = os.path.join(data_dir, "config", "chronicle")
    guts["localtoken_path"] = os.path.join(data_dir, "localtoken")

    rpd = read_guts(guts, "relative_prom_stats_dir")
    guts["prom_stats_dir"] = os.path.join(data_dir, rpd)


def add_bucket_info(guts):
    # Assume directories in the data directory are for buckets unless
    # they have invalid bucket names.
    dbdir = os.path.realpath(read_guts(guts, "db_dir"))
    buckets = []
    try:
        buckets = [f.name for f in os.scandir(dbdir)
                   if f.is_dir() and not f.name.startswith(('.', '@'))]
    except IOError as e:
        log("add_bucket_info failed to walk data dir " +
            "I/O error(%s): %s directory:%s" % (e.errno, e.strerror, dbdir))

    log(f"Adding persistent buckets '{buckets}' to server guts")
    guts["persistent_buckets"] = buckets


def add_db_idx_dirs(guts):
    couch_ini_files = read_guts(guts, "couch_inis").split(";"),
    for file in couch_ini_files:
        config = configparser.ConfigParser()
        config.read(file)
        try:
            guts["db_dir"] = config['couchdb']['database_dir']
        except KeyError:
            pass
        try:
            guts["idx_dir"] = config['couchdb']['view_index_dir']
        except KeyError:
            pass


def winquote_path(s):
    return '"' + s.replace("\\\\", "\\").replace('/', "\\") + '"'

# python's split splits empty string to [''] which doesn't make any
# sense. So this function works around that.


def correct_split(string, splitchar):
    rv = string.split(splitchar)
    if rv == ['']:
        rv = []
    return rv


def make_product_task(guts, initargs_path, memcached_pass, options):
    if read_guts(guts, "tls") == "true":
        tls = True
    else:
        tls = False
    root = os.path.abspath(os.path.join(initargs_path, "..", "..", "..", ".."))
    dbdir = os.path.realpath(read_guts(guts, "db_dir"))
    viewdir = os.path.realpath(read_guts(guts, "idx_dir"))
    rebdir = os.path.realpath(os.path.join(
        read_guts(guts, "log_path"), "rebalance"))
    nodes = correct_split(read_guts(guts, "nodes"), ",")

    diag_url = "http://%s:%s/diag" % (
        local_url_addr, read_guts(guts, "rest_port"))

    from distutils.spawn import find_executable

    lookup_cmd = None
    for cmd in ["dig", "nslookup", "host"]:
        if find_executable(cmd) is not None:
            lookup_cmd = cmd
            break

    lookup_tasks = []
    if lookup_cmd is not None:
        if lookup_cmd == "nslookup":
            lookup_tasks = [AllOsTask(f"DNS lookup information for {node}",
                                      f"{lookup_cmd} '{node}'")
                            for node in nodes]
        else:
            lookup_tasks = [UnixTask(f"DNS lookup information for {node}",
                                     f"{lookup_cmd} '{node}'")
                            for node in nodes]

    getent_tasks = [LinuxTask("Name Service Switch "
                              "hosts database info for %s" % node,
                              ["getent", "ahosts", node])
                    for node in nodes]

    query_tasks = []
    query_port = read_guts(guts, "query_port")
    if query_port:
        def make(statement, logfile=DEFAULT_LOG, **kwargs):
            return make_query_statement_task(statement, user="@",
                                             password=memcached_pass,
                                             port=query_port,
                                             logfile=logfile,
                                             **kwargs)

        query_tasks = [
            make("SELECT * FROM system:datastores"),
            make("SELECT * FROM system:namespaces"),
            make("SELECT * FROM system:keyspaces"),
            make("SELECT * FROM system:indexes"),
            make('SELECT * FROM system:functions'),
            make('SELECT requests.*, '
                 '"<ud>"||requests.statement||"</ud>" AS statement, '
                 '"<ud>"||requests.preparedText||"</ud>" AS preparedText, '
                 '"<ud>"||encode_json(meta().plan)||"</ud>" AS plan, '
                 '"<ud>"||encode_json(requests.namedArgs)||"</ud>" AS namedArgs, '
                 '"<ud>"||encode_json(requests.positionalArgs)||"</ud>" AS positionalArgs, '
                 '"<ud>"||requests.users||"</ud>" AS users '
                 'FROM system:completed_requests AS requests;',
                 logfile="completed_requests.json", no_header=True, timeout=60*5),
            make('SELECT prepareds.*, '
                 '"<ud>"||prepareds.statement||"</ud>" AS statement '
                 'FROM system:prepareds AS prepareds;',
                 logfile="prepareds.json", no_header=True),
            *make_golang_profile_tasks('Query', memcached_pass, query_port,
                                       'query')
        ]

    index_tasks = []
    index_port = read_guts(guts, "indexer_http_port")
    if index_port:
        index_tasks = [
            make_index_task(
                "Index definitions are: ",
                "getIndexStatus",
                memcached_pass,
                index_port),
            make_index_task(
                "Indexer settings are: ",
                "settings",
                memcached_pass,
                index_port),
            make_index_task(
                "Indexer stats are: ",
                "stats?partition=true",
                memcached_pass,
                index_port),
            make_index_task(
                "Index storage stats are: ",
                "stats/storage",
                memcached_pass,
                index_port),
            make_index_task(
                "MOI allocator stats are: ",
                "stats/storage/mm",
                memcached_pass,
                index_port),
            make_index_task(
                "Indexer Rebalance Tokens: ",
                "listRebalanceTokens",
                memcached_pass,
                index_port),
            make_index_task(
                "Indexer Metadata Tokens: ",
                "listMetadataTokens",
                memcached_pass,
                index_port),
            *make_golang_profile_tasks('Indexer', memcached_pass, index_port,
                                       'indexer')
        ]

    projector_tasks = []
    proj_port = read_guts(guts, "projector_port")
    if proj_port:
        projector_tasks = [
            *make_golang_profile_tasks('Projector', memcached_pass, proj_port,
                                       'projector', tls=tls)
        ]

    goxdcr_tasks = []
    goxdcr_port = read_guts(guts, "xdcr_rest_port")
    if goxdcr_port:
        goxdcr_url = f'http://{local_url_addr}:{goxdcr_port}/pools/default'
        redact_opt = ""
        if options.redact_level != "none":
            redact_opt = '?redactRequested=true'
        rc_url = f'{goxdcr_url}/remoteClusters{redact_opt}'
        rp_url = f'{goxdcr_url}/replications{redact_opt}'
        goxdcr_tasks = [
            *make_golang_profile_tasks('GoXDCR', memcached_pass, goxdcr_port,
                                       'goxdcr'),
            make_curl_task(name='GoXDCR RemoteClusters: ',
                           user="@", password=memcached_pass,
                           url=rc_url, timeout=300,
                           log_file="goxdcr_remote_clusters.json",
                           no_header=True),
            make_curl_task(name='GoXDCR Replications: ',
                           user="@", password=memcached_pass,
                           url=rp_url, timeout=300,
                           log_file="goxdcr_replications.json",
                           no_header=True)
        ]

    fts_tasks = []
    fts_port = read_guts(guts, "fts_http_port")
    if fts_port:
        url = 'http://%s:%s/api/diag' % (local_url_addr, fts_port)
        fts_tasks = [
            make_curl_task(name="FTS /api/diag: ",
                           user="@", password=memcached_pass,
                           url=url,
                           log_file="fts_diag.json", no_header=True),
            *make_golang_profile_tasks('FTS', memcached_pass, fts_port,
                                       'fts')
        ]

    cbas_tasks = []
    cbas_port = read_guts(guts, "cbas_http_port")
    if cbas_port:
        cbas_diag_url = 'http://%s:%s/analytics/node/diagnostics' % (
            local_url_addr, cbas_port)
        cbas_parent_port = read_guts(guts, "cbas_parent_port")

        def make_cbas(statement):
            return make_cbas_statement_task(statement, user="@",
                                            password=memcached_pass,
                                            port=cbas_port)

        cbas_tasks = [
            make_curl_task(
                name="Analytics /analytics/node/diagnostics: ",
                user="@",
                password=memcached_pass,
                url=cbas_diag_url,
                log_file="analytics_diag.json",
                no_header=True),
            make_cbas("select * from `Metadata`.`Dataverse`"),
            make_cbas("select * from `Metadata`.`Dataset`"),
            make_cbas("select * from `Metadata`.`Index`"),
            make_cbas("select * from `Metadata`.`Bucket`"),
            make_cbas("select * from `Metadata`.`Link`"),
            *make_golang_profile_tasks('Analytics', memcached_pass,
                                       cbas_parent_port, 'analytics')
        ]

    eventing_tasks = []
    eventing_port = read_guts(guts, "eventing_http_port")
    if eventing_port:
        stats_url = 'http://%s:%s/api/v1/stats?type=full' % (
            local_url_addr, eventing_port)
        eventing_insight_url = 'http://%s:%s/getInsight?udmark=true&aggregate=false' % (
            local_url_addr, eventing_port)
        eventing_tasks = [
            make_curl_task(
                name="Eventing /api/v1/stats: ",
                user="@",
                password=memcached_pass,
                url=stats_url,
                log_file="eventing_stats.json",
                no_header=True),
            make_curl_task(
                name="Eventing code insights: ",
                user="@",
                password=memcached_pass,
                url=eventing_insight_url,
                log_file="eventing_insights.log",
                no_header=True),
            *make_golang_profile_tasks('Eventing', memcached_pass,
                                       eventing_port, 'eventing')
        ]

    backup_tasks = []
    backup_port = read_guts(guts, "backup_http_port")
    if backup_port:
        backup_tasks = [
            make_curl_task(
                name="Backup service information: ",
                user="@",
                password=memcached_pass,
                url=f"http://{local_url_addr}:{backup_port}/internal/v1/serviceInfo",
            ),
            *
            make_golang_profile_tasks(
                'Backup',
                memcached_pass,
                backup_port,
                'backup')]

    _tasks = [
        AllOsTask("Phosphor Trace",
                  ["kv_trace_dump",
                   "-H", "%s:%s" % (local_url_addr,
                                    read_guts(guts, "memcached_dedicated_port")),
                   "-u", read_guts(guts, "memcached_admin"),
                   "kv_trace.json"],
                  timeout=120,
                  log_file="stats.log",
                  change_dir=True,
                  artifacts=["kv_trace.json"],
                  addenv=[("CB_PASSWORD", memcached_pass)]),
        UnixTask("Directory structure",
                 ["ls", "-lRai", root]),
        UnixTask("Database directory structure",
                 ["ls", "-lRai", dbdir]),
        UnixTask("Index directory structure",
                 ["ls", "-lRai", viewdir]),
        UnixTask("couch_dbinfo",
                 ["find", dbdir, "-type", "f",
                  "-name", "*.couch.*",
                  "-exec", "couch_dbinfo", "{}", "+"]),
        LinuxTask("Database directory filefrag info",
                  ["find", dbdir, "-type", "f", "-exec", "filefrag", "-v", "{}", "+"]),
        LinuxTask("Index directory filefrag info",
                  ["find", viewdir, "-type", "f", "-exec", "filefrag", "-v", "{}", "+"]),
        WindowsTask("Directory structure",
                    "dir /s " + winquote_path(root)),
        WindowsTask("Database directory structure",
                    "dir /s " + winquote_path(dbdir)),
        WindowsTask("Index directory structure",
                    "dir /s " + winquote_path(viewdir)),
        WindowsTask("Version file",
                    "type " + winquote_path(basedir()) + "\\VERSION.txt"),
        WindowsTask("Manifest file",
                    "type " + winquote_path(basedir()) + "\\manifest.txt"),
        WindowsTask("Manifest file",
                    "type " + winquote_path(basedir()) + "\\manifest.xml"),
        LinuxTask("Version file", "cat '%s/VERSION.txt'" % root),
        LinuxTask("Variant file", "cat '%s/VARIANT.txt'" % root),
        LinuxTask("Manifest file", "cat '%s/manifest.txt'" % root),
        LinuxTask("Manifest file", "cat '%s/manifest.xml'" % root),
        LiteralTask("Couchbase config", read_guts(guts, "ns_config")),
        LiteralTask("Couchbase static config",
                    read_guts(guts, "static_config")),
        # TODO: just gather those in python
        WindowsTask("Memcached logs",
                    "cd " + winquote_path(read_guts(guts, "memcached_logs_path")) + " && " +
                    "for /f %a IN ('dir memcached.log.* /od /tw /b') do type %a",
                    log_file="memcached.log"),
        UnixTask("Memcached logs",
                 ["sh", "-c", 'cd "$1"; for file in $(ls -tr memcached.log.*); do cat \"$file\"; done',
                  "--", read_guts(guts, "memcached_logs_path")],
                 log_file="memcached.log"),
        [WindowsTask("Ini files (%s)" % p,
                     "type " + winquote_path(p),
                     log_file="ini.log")
         for p in read_guts(guts, "couch_inis").split(";")],
        UnixTask("Ini files",
                 ["sh", "-c", 'for i in "$@"; do echo "file: $i"; cat "$i"; done',
                     "--"] + read_guts(guts, "couch_inis").split(";"),
                 log_file="ini.log"),
        make_curl_task(name="couchbase diags",
                       user="@",
                       password=memcached_pass,
                       timeout=600,
                       url=diag_url,
                       log_file="diag.log"),

        make_curl_task(name="master events",
                       user="@",
                       password=memcached_pass,
                       timeout=300,
                       url='http://%s:%s/diag/masterEvents?o=1' % (
                           local_url_addr, read_guts(guts, "rest_port")),
                       log_file="master_events.log",
                       no_header=True),

        make_curl_task(name="ale configuration",
                       user="@",
                       password=memcached_pass,
                       url='http://%s:%s/diag/ale' % (
                           local_url_addr, read_guts(guts, "rest_port")),
                       log_file=DEFAULT_LOG),

        [AllOsTask("couchbase logs (%s)" % name, "cbbrowse_logs %s" % name,
                   addenv=[("REPORT_DIR", read_guts(guts, "log_path"))],
                   log_file="ns_server.%s" % name)
         for name in ["debug.log", "info.log", "error.log", "couchdb.log",
                      "xdcr_target.log", "prometheus.log",
                      "views.log", "mapreduce_errors.log",
                      "stats.log", "babysitter.log",
                      "reports.log", "http_access.log",
                      "http_access_internal.log", "ns_couchdb.log",
                      "goxdcr.log", "query.log", "projector.log", "indexer.log",
                      "fts.log", "metakv.log", "json_rpc.log", "eventing.log",
                      "analytics_info.log", "analytics_debug.log", "analytics_shutdown.log",
                      "analytics_error.log", "analytics_warn.log", "analytics_dcpdebug.log",
                      "analytics_trace.json", "analytics_access.log", "analytics_cbas_debug.log",
                      "indexer_stats.log", "backup_service.log", "key.log"]],

        [make_cbstats_all_buckets_task(kind, memcached_pass, guts)
         for kind in ["all", "checkpoint", "collections", "config",
                      "dcp", "dcpagg",
                      ["diskinfo", "detail"], ["dispatcher", "logs"],
                      "eviction", "failovers",
                      "kvstore", "kvtimings", "memory",
                      "prev-vbucket", "range-scans", ["responses", "all"],
                      "runtimes", "scheduler", "scopes",
                      "timings", "uuid",
                      "vbucket-details", "vbucket-seqno",
                      "warmup", "workload"]],

        [make_cbstats_task(kind, memcached_pass, guts)
         for kind in ["tasks-all"]],

        [AllOsTask("memcached mcstat %s" % kind,
                   flatten(["mcstat", "-h",
                            "%s:%s" % (local_url_addr,
                                       read_guts(guts,
                                                 "memcached_dedicated_port")),
                            "-u", read_guts(guts, "memcached_admin"), kind]),
                   log_file="stats.log",
                   timeout=60,
                   addenv=[("CB_PASSWORD", memcached_pass)])
         for kind in ["allocator", "clocks", "connections", "threads",
                      "tracing"]],

        # mcstat -a (iterate for all buckets)
        [AllOsTask("memcached mcstat %s" % kind,
                   flatten(["mcstat", "-a", "-h",
                            "%s:%s" % (local_url_addr,
                                       read_guts(guts,
                                                 "memcached_dedicated_port")),
                            "-u", read_guts(guts, "memcached_admin"), kind]),
                   log_file="stats.log",
                   timeout=60,
                   addenv=[("CB_PASSWORD", memcached_pass)])
         for kind in ["collections-details"]],

        [AllOsTask("fts scorch zap (%s)" % path,
                   ["cbft-bleve", "zap", "v11", "footer", path],
                   log_file="fts_store_stats.log")
         for path in glob.glob(os.path.join(viewdir, "@fts", "*.pindex", "store", "*.zap"))],

        [AllOsTask("fts scorch zap (%s)" % path,
                   ["cbft-bleve", "zap", "v12", "footer", path],
                   log_file="fts_store_stats.log")
         for path in glob.glob(os.path.join(viewdir, "@fts", "*.pindex", "store", "*.zap"))],

        [AllOsTask("fts scorch zap (%s)" % path,
                   ["cbft-bleve", "zap", "v13", "footer", path],
                   log_file="fts_store_stats.log")
         for path in glob.glob(os.path.join(viewdir, "@fts", "*.pindex", "store", "*.zap"))],

        [AllOsTask("fts scorch zap (%s)" % path,
                   ["cbft-bleve", "zap", "v14", "footer", path],
                   log_file="fts_store_stats.log")
         for path in glob.glob(os.path.join(viewdir, "@fts", "*.pindex", "store", "*.zap"))],

        [AllOsTask("fts scorch zap (%s)" % path,
                   ["cbft-bleve", "zap", "v15", "footer", path],
                   log_file="fts_store_stats.log")
         for path in glob.glob(os.path.join(viewdir, "@fts", "*.pindex", "store", "*.zap"))],

        [AllOsTask("ddocs for %s (%s)" % (bucket, path),
                   ["couch_dbdump", path],
                   log_file="ddocs.log")
         for bucket in read_guts(guts, "persistent_buckets")
         for path in glob.glob(os.path.join(dbdir, bucket, "master.couch*"))],

        [AllOsTask("Couchstore local documents (%s, %s)" % (bucket, os.path.basename(path)),
                   ["couch_dbdump", "--local", path],
                   log_file="couchstore_local.log")
         for bucket in read_guts(guts, "persistent_buckets")
         for path in glob.glob(os.path.join(dbdir, bucket, "*.couch.*"))],

        [AllOsTask("mdocs for %s (%s)" % (bucket, path),
                   ["magma_dump", path, "--cbcollect"],
                   log_file="mdocs.log")
         for bucket in read_guts(guts, "persistent_buckets")
         for path in glob.glob(os.path.join(dbdir, bucket))],

        # RocksDB has logs per DB (i.e. vBucket). 'LOG' is the most
        # recent file, with old files named LOG.old.<timestamp>.
        # Sort so we go from oldest -> newest as per other log files.
        [AllOsTask("RocksDB Log file (%s, %s)" % (bucket, os.path.basename(path)),
                   "cat '%s'" % (log_file),
                   log_file="kv_rocks.log")
         for bucket in read_guts(guts, "persistent_buckets")
         for path in glob.glob(os.path.join(dbdir, bucket, "rocksdb.*"))
         for log_file in sorted(glob.glob(os.path.join(path, "LOG.old.*"))) + [os.path.join(path, "LOG")]],

        [AllOsTask("mctimings %s" % stat,
                   ["mctimings",
                    "-u", read_guts(guts, "memcached_admin"),
                    "-h", "%s:%s" % (local_url_addr,
                                     read_guts(guts, "memcached_dedicated_port")),
                    "-a", "-v"] + stat,
                   log_file="stats.log",
                   timeout=60,
                   addenv=[("CB_PASSWORD", memcached_pass)])
         for stat in ([], ["subdoc_execute", "snappy_decompress", "json_validate"])],

        CollectFileTask(
            "Users storage", read_guts(
                guts, "users_storage_path")),

        CollectFileTask(
            "Dist configuration (dist_cfg)", read_guts(
                guts, "dist_cfg_path")),

        [CollectFileTask("Rebalance Report: %s" % path, path)
         for path in glob.glob(os.path.join(rebdir, "rebalance_report*"))],

        CollectFileTask("NS Log", read_guts(guts, "ns_log_path")),
        CollectFileTask("Event Logs", read_guts(guts, "event_log_path")),
        # MB-42657: For signal safety memcached writes crash output to a
        # separate file with signal safe functions. At restart memcached will
        # log the contents and discard the file. If there is no restart the file
        # must be captured.
        CollectFileTask("Memcached breakpad output",
                        os.path.join(read_guts(guts, "memcached_logs_path"),
                                     "memcached.log.breakpad.crash.txt")),
    ]

    _tasks = flatten([getent_tasks,
                      lookup_tasks,
                      make_chronicle_snapshots_task(guts, initargs_path),
                      make_chronicle_logs_task(guts, initargs_path),
                      query_tasks,
                      index_tasks,
                      projector_tasks,
                      fts_tasks,
                      cbas_tasks,
                      eventing_tasks,
                      goxdcr_tasks,
                      backup_tasks,
                      _tasks])

    return _tasks


def find_script(name):
    path = os.path.join(basedir(), "bin", name)
    if os.path.exists(path):
        log("Found %s: %s" % (name, path))
        return os.path.abspath(path)

    log(f"Could not find script {name}")
    return None


def get_server_guts(initargs_path):
    dump_guts_path = find_script("dump-guts")

    if dump_guts_path is None:
        log("Couldn't find dump-guts script. Some information will be missing")
        return {}

    # Check if initargs exists and is read accessible.
    if not os.path.exists(initargs_path):
        log("initargs file '{}' does not exist".format(initargs_path))
        return {}

    if not os.access(initargs_path, os.R_OK):
        log("Read access to '{}' is required in order to proceed".format(
            initargs_path))
        sys.exit(1)

    escript = exec_name("escript")
    extra_args = os.getenv("EXTRA_DUMP_GUTS_ARGS")
    args = [escript, dump_guts_path, "--initargs-path", initargs_path]
    if extra_args:
        args = args + extra_args.split(";")
    print("Checking for server guts in %s..." % initargs_path)
    p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    output, err = p.communicate()
    p.wait()
    rc = p.returncode
    d = {}
    if rc != 0:
        log(f"Error occurred getting server guts: {err.decode(LATIN1)}")
        return d
    # print("args: %s gave rc: %d and:\n\n%s\n" % (args, rc, output))
    tokens = output.decode(LATIN1).rstrip("\0").split("\0")
    if len(tokens) > 1:
        for i in range(0, len(tokens), 2):
            d[tokens[i]] = tokens[i + 1]
    return d


def guess_utility(command):
    if isinstance(command, list):
        command = ' '.join(command)

    if not command:
        return None

    if re.findall(r'[|;&]|\bsh\b|\bsu\b|\bfind\b|\bfor\b', command):
        # something hard to easily understand; let the human decide
        return command
    else:
        return command.split()[0]


def dump_utilities(*args, **kwargs):
    specific_platforms = {SolarisTask: 'Solaris',
                          LinuxTask: 'Linux',
                          WindowsTask: 'Windows',
                          MacOSXTask: 'Mac OS X'}
    platform_utils = dict((name, set())
                          for name in specific_platforms.values())

    class FakeOptions(object):
        def __getattr__(self, name):
            return None

    tasks = make_os_tasks() + make_product_task({}, "", "", FakeOptions())

    for task in tasks:
        utility = guess_utility(task.command)
        if utility is None:
            continue

        for (platform, name) in specific_platforms.items():
            if isinstance(task, platform):
                platform_utils[name].add(utility)

    print('This is an autogenerated, possibly incomplete and flawed list '
          'of utilites used by cbcollect_info')

    for (name, utilities) in sorted(
            platform_utils.items(), key=lambda x: x[0]):
        print("\n%s:" % name)

        for utility in sorted(utilities):
            print("        - %s" % utility)

    sys.exit(0)


def stdin_watcher():
    fd = sys.stdin.fileno()

    while True:
        buf = os.read(fd, 1024)
        # stdin closed
        if not buf:
            break


def setup_stdin_watcher():
    def _in_thread():
        try:
            stdin_watcher()
        finally:
            AltExit.exit(2)
    th = threading.Thread(target=_in_thread, daemon=True)
    th.start()


class CurlKiller:
    def __init__(self, p):
        self.p = p

    def cleanup(self):
        if self.p is not None:
            print("Killing curl...")
            os.kill(self.p.pid, signal.SIGKILL)
            print("done")

    def disarm(self):
        self.p = None


def do_upload_and_exit(path, url, proxy, tmp_dir=None):
    output_fd, output_file = tempfile.mkstemp(dir=tmp_dir)
    os.close(output_fd)

    AltExit.register(lambda: os.unlink(output_file))

    args = ["curl", "-sS",
            "--output", output_file,
            "--proxy", proxy,
            "--write-out", "%{http_code}", "--upload-file", path, url]
    AltExit.lock.acquire()
    try:
        p = subprocess.Popen(args, stdout=subprocess.PIPE)
        k = CurlKiller(p)
        AltExit.register_and_unlock(k.cleanup)
    except Exception as e:
        AltExit.lock.release()
        raise e

    stdout, _ = p.communicate()
    stdout = stdout.decode(LATIN1)
    k.disarm()

    if p.returncode != 0:
        sys.exit(1)
    else:
        if stdout.strip() == '200':
            log('Upload path is: %s' % url)
            log('Done uploading')
            sys.exit(0)
        else:
            log('HTTP status code: %s' % stdout)
            sys.exit(1)


def parse_host(host):
    url = urllib.parse.urlsplit(host)
    if not url.scheme:
        url = urllib.parse.urlsplit('https://' + host)

    return url.scheme, url.netloc, url.path


def generate_upload_url(parser, options, zip_filename):
    upload_url = None
    if options.upload_host:
        if not options.upload_customer:
            parser.error("Need --customer when --upload-host is given")

        scheme, netloc, path = parse_host(options.upload_host)

        customer = urllib.parse.quote(options.upload_customer)
        fname = urllib.parse.quote(os.path.basename(zip_filename))
        if options.upload_ticket:
            full_path = '%s/%s/%d/%s' % (path,
                                         customer,
                                         options.upload_ticket,
                                         fname)
        else:
            full_path = '%s/%s/%s' % (path, customer, fname)

        upload_url = urllib.parse.urlunsplit(
            (scheme, netloc, full_path, '', ''))
        log("Will upload collected .zip file into %s" % upload_url)
    return upload_url


def check_ticket(option, opt, value):
    if re.match(r'^\d{1,7}$', value):
        return int(value)
    else:
        raise optparse.OptionValueError(
            "option %s: invalid ticket number: %r" % (opt, value))


class CbcollectInfoOptions(optparse.Option):
    from copy import copy

    TYPES = optparse.Option.TYPES + ("ticket",)
    TYPE_CHECKER = copy(optparse.Option.TYPE_CHECKER)
    TYPE_CHECKER["ticket"] = check_ticket


def cleanup_old_prometheus_stats(guts):
    prom_stats_dir = read_guts(guts, "prom_stats_dir")
    if not prom_stats_dir:
        return None

    # Remove any existing snapshot directory. Assumes we're the only client
    # of prometheus snapshots.
    snapshot_dir = os.path.join(prom_stats_dir, "snapshots")
    log(f"Removing existing snapshot directory '{snapshot_dir}'")
    shutil.rmtree(snapshot_dir, ignore_errors=True)


def post_rest_api(description, api, guts):
    token = get_local_token(guts)
    port = read_guts(guts, "rest_port")
    user_and_password = f"@localtoken:{token}"
    url = f"http://{local_url_addr}:{port}/{api}"

    command = ["curl", "-X", "POST", "-sS", "--proxy", "", "--fail",
               "-u", user_and_password, url]
    # Don't append a newline to the result, since that would corrupt
    # the snapshot.
    task = AllOsTask(description, command, timeout=60, no_header=True,
                     suppress_append_newline=True)
    output_bytes = BytesIO()
    status = task.execute(output_bytes)
    if status != 0:
        log(f"Failed rest request {url}: {status}")
        return None
    return output_bytes.getvalue().decode(LATIN1)


def export_chronicle_snapshot(guts):
    # The path of the created snapshot directory is returned.
    return post_rest_api("Export chronicle snapshot",
                         "_exportChronicleSnapshot", guts)


def get_chronicle_snapshot_dir(guts):
    path = export_chronicle_snapshot(guts)
    if path is not None and os.path.exists(os.path.join(path, "kv.snapshot")):
        return path
    # We expect this code path to trigger when the server isn't running.
    snapshots_path = os.path.join(
        read_guts(
            guts,
            "chronicle_dir"),
        "snapshots")
    try:
        max_seqno = max([int(i) for i in os.listdir(snapshots_path)])
        return os.path.join(snapshots_path, str(max_seqno))
    except BaseException:
        return None


def get_prometheus_stats_via_snapshot(guts):
    # Generate a prometheus snapshot. These consist of hard links to
    # existing blocks and a dump of the current open blocks.
    # The name of the created snapshot directory is returned.
    snapshot_dir_path = post_rest_api("Generate prometheus snapshot",
                                      "_createStatsSnapshot", guts)
    return snapshot_dir_path


def get_prometheus_stats_from_disk(guts):
    prom_stats_dir = read_guts(guts, "prom_stats_dir")
    if not prom_stats_dir:
        return None

    # As a prometheus snapshot couldn't be obtained we're going to
    # grab the contents of the directory.  Typically this is much
    # larger than a snapshot so we'll try to do what we can to
    # decrease the size.

    # Until we can determine how to do so we'll just return the entire
    # directory

    return prom_stats_dir


def get_prometheus_stats(guts):
    snapshot_dir_path = get_prometheus_stats_via_snapshot(guts)
    if snapshot_dir_path is not None and os.path.exists(snapshot_dir_path):
        return snapshot_dir_path

    log("Failed to get prometheus snapshot. Will attempt to get stats "
        "from disk.")

    return get_prometheus_stats_from_disk(guts)


def main():
    # ask all tools to use C locale (MB-12050)
    os.environ['LANG'] = 'C'
    os.environ['LC_ALL'] = 'C'
    if 'HOME' not in os.environ:
        os.environ['HOME'] = basedir()

    rootdir = basedir()
    # (MB-8239)erl script fails in OSX as it is unable to find COUCHBASE_TOP -ravi
    if platform.system() == 'Darwin':
        os.environ["COUCHBASE_TOP"] = rootdir

    parser = optparse.OptionParser(
        usage=USAGE, option_class=CbcollectInfoOptions)
    parser.add_option("-r", dest="root",
                      help="root directory - defaults to %s" % (rootdir),
                      default=rootdir)
    parser.add_option("-v", dest="verbosity", help="increase verbosity level",
                      action="count", default=0)
    parser.add_option(
        "-p",
        dest="product_only",
        help="gather only product related information",
        action="store_true",
        default=False)
    parser.add_option("-d", action="callback", callback=dump_utilities,
                      help="dump a list of commands that cbcollect_info needs")
    parser.add_option("--watch-stdin", dest="watch_stdin",
                      action="store_true", default=False,
                      help=optparse.SUPPRESS_HELP)
    parser.add_option("--initargs", dest="initargs",
                      help="server 'initargs' path")
    parser.add_option(
        "--log-redaction-level",
        dest="redact_level",
        default="none",
        help="redaction level for the logs collected, none and partial supported (default is none)")
    parser.add_option("--log-redaction-salt", dest="salt_value",
                      default=str(uuid.uuid4()),
                      help="Is used to salt the hashing of tagged data, \
                            defaults to random uuid. If input by user it should \
                            be provided along with --log-redaction-level option")
    parser.add_option("--just-upload-into", dest="just_upload_into",
                      help=optparse.SUPPRESS_HELP)
    parser.add_option(
        "--upload-host",
        dest="upload_host",
        help="gather diagnostics and upload it for couchbase support. Gives upload host")
    parser.add_option("--customer", dest="upload_customer",
                      help="specifies customer name for upload")
    parser.add_option("--ticket", dest="upload_ticket", type='ticket',
                      help="specifies support ticket number for upload")
    parser.add_option("--bypass-sensitive-data", dest="bypass_sensitive_data",
                      action="store_true", default=False,
                      help="do not collect sensitive data")
    parser.add_option(
        "--task-regexp",
        dest="task_regexp",
        default="",
        help="Run only tasks matching regexp. For debugging purposes only.")
    parser.add_option(
        "--tmp-dir",
        dest="tmp_dir",
        default=None,
        help="set the temp dir used while processing collected data. Overrides the TMPDIR env variable if set")
    parser.add_option("--upload-proxy", dest="upload_proxy", default="",
                      help="specifies proxy for upload")
    options, args = parser.parse_args()

    if len(args) != 1:
        parser.error(
            "incorrect number of arguments. Expecting filename to collect diagnostics into")

    if options.watch_stdin:
        setup_stdin_watcher()

    zip_filename = args[0]
    if zip_filename[-4:] != '.zip':
        zip_filename = zip_filename + '.zip'

    zip_dir = os.path.dirname(os.path.abspath(zip_filename))

    if not os.access(zip_dir, os.W_OK | os.X_OK):
        log("do not have write access to the directory %s" % (zip_dir))
        sys.exit(1)

    cur_work_dir = os.getcwd()
    if not os.access(cur_work_dir, os.R_OK | os.X_OK):
        log("Read/execute access to current working directory '{}' is "
            "required".format(cur_work_dir))
        sys.exit(1)

    if options.redact_level != "none" and options.redact_level != "partial":
        parser.error(
            "Invalid redaction level. Only 'none' and 'partial' are supported.")

    redact_zip_file = zip_filename[:-4] + "-redacted" + zip_filename[-4:]
    upload_url = ""
    if options.redact_level != "none":
        upload_url = generate_upload_url(parser, options, redact_zip_file)
    else:
        upload_url = generate_upload_url(parser, options, zip_filename)

    bindir = os.path.join(options.root, 'bin')
    if os.name == 'posix':
        path = [bindir,
                '/opt/couchbase/bin',
                os.environ['PATH'],
                '/bin',
                '/sbin',
                '/usr/bin',
                '/usr/sbin']
        os.environ['PATH'] = ':'.join(path)

        library_path = [os.path.join(options.root, 'lib')]

        current_library_path = os.environ.get('LD_LIBRARY_PATH')
        if current_library_path is not None:
            library_path.append(current_library_path)

        os.environ['LD_LIBRARY_PATH'] = ':'.join(library_path)
    elif os.name == 'nt':
        path = [bindir, os.environ['PATH']]
        os.environ['PATH'] = ';'.join(path)

    if options.just_upload_into is not None:
        do_upload_and_exit(args[0], options.just_upload_into,
                           options.upload_proxy, tmp_dir=options.tmp_dir)

    # We want this at the top of couchbase.log
    all_tasks = []
    all_tasks.append(make_redaction_task())

    if not options.product_only:
        all_tasks.extend(make_os_tasks())

    initargs_variants = [
        os.path.abspath(
            os.path.join(
                options.root,
                "var",
                "lib",
                "couchbase",
                "initargs")),
        "/opt/couchbase/var/lib/couchbase/initargs",
        os.path.expanduser("~/Library/Application Support/Couchbase/var/lib/couchbase/initargs")]

    if options.initargs is not None:
        initargs_variants = [options.initargs]

    guts = None
    guts_initargs_path = None

    for initargs_path in initargs_variants:
        d = get_server_guts(initargs_path)
        if len(d) > 0:
            guts = d
            guts_initargs_path = os.path.abspath(initargs_path)
            break

    prefix = None
    if guts is None:
        log("Couldn't read server guts. Using some default values.")

        if platform.system() == 'Windows':
            prefix = 'c:/Program Files/Couchbase/Server'
        elif platform.system() == 'Darwin':
            prefix = '~/Library/Application Support/Couchbase'
        else:
            prefix = '/opt/couchbase'

        guts = {
            "db_dir": os.path.join(
                prefix,
                "var/lib/couchbase/data"),
            "idx_dir": os.path.join(
                prefix,
                "var/lib/couchbase/data"),
            "ns_log_path": os.path.join(
                prefix,
                "var/lib/couchbase/ns_log"),
            "event_log_path": os.path.join(
                prefix,
                "var/lib/couchbase/event_log"),
            "log_path": os.path.join(
                prefix,
                "var/lib/couchbase/logs"),
            "memcached_logs_path": os.path.join(
                prefix,
                "var/lib/couchbase/logs")}

        guts_initargs_path = os.path.abspath(prefix)

    ipv6 = read_guts(guts, "ipv6") == "true"
    set_local_addr(ipv6)

    populate_guts_with_additional_info(guts)

    memcached_password = get_diag_password(guts)
    zip_node = read_guts(guts, "node")
    zip_filename = args[0]
    if zip_filename[-4:] != ".zip":
        zip_filename = f"{zip_filename}.zip"

    # Salt value is going to represent redaction, to some extent. We don't
    # need both variables for our needs but we need to just leave salt_value
    # empty if we don't want redaction.
    salt_value = ""
    if options.redact_level != "none":
        log("Redacting log files to level: %s" % options.redact_level)
        salt_value = options.salt_value

    start_time = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
    folder_name = f"cbcollect_info_{zip_node}_{start_time}"
    runner = TaskRunner(zip_filename, prefix=folder_name,
                        salt_value=salt_value,
                        tmp_dir=options.tmp_dir,
                        verbosity=options.verbosity,
                        task_regexp=options.task_regexp)

    diag_header_task = runner.literal_task(
        "product diag header", "Found server initargs at %s (%d)" %
        (guts_initargs_path, len(guts)))
    all_tasks.append(diag_header_task)
    all_tasks.extend(make_product_task(guts, guts_initargs_path,
                                       memcached_password, options))

    for f in glob.glob(os.path.join(guts.get("path_config_datadir"),
                                    "config", "certs", "*")):
        base = os.path.basename(f)
        if base not in ["pkey.pem", "client_pkey.pem"]:
            all_tasks.append(CollectFileTask(f"Collecting {base}", f,
                                             f"certs/{base}"))

    # Collect breakpad crash dumps.
    if options.bypass_sensitive_data:
        log("Bypassing Sensitive Data: Breakpad crash dumps")
    else:
        memcached_breakpad_minidump_dir = read_guts(
            guts, "memcached_breakpad_minidump_dir")
        for dump in glob.glob(os.path.join(
                memcached_breakpad_minidump_dir, "*.dmp")):
            all_tasks.append(CollectFileTask(f"Collecting file {dump}", dump))

        # Collect indexer breakpad minidumps
        index_port = read_guts(guts, "indexer_http_port")
        if index_port:
            indexer_breakpad_minidump_dir = read_guts(
                guts, "indexer_breakpad_minidump_dir")
            if memcached_breakpad_minidump_dir != indexer_breakpad_minidump_dir:
                for dump in glob.glob(os.path.join(
                        indexer_breakpad_minidump_dir, "*.dmp")):
                    all_tasks.append(
                        CollectFileTask(
                            f"Collecting file {dump}", dump))

    # Collect ASan / UBSan log files (sanitized builds)
    for sanitizer_log in glob.glob(os.path.join(
            read_guts(guts, "log_path"), "sanitizers.log.*")):
        all_tasks.append(CollectFileTask(f"Collecting file {sanitizer_log}",
                                         sanitizer_log))

    # Collect prometheus stats files
    cleanup_old_prometheus_stats(guts)
    snapshot_dir_path = get_prometheus_stats(guts)
    if snapshot_dir_path is not None:
        tasks = CollectFileTask.create_directory_collection_tasks(
            snapshot_dir_path, "stats_snapshot")
        all_tasks.extend(tasks)
    else:
        log("Error: unable to retrieve statistics")

    fts_port = read_guts(guts, "fts_http_port")
    if fts_port:
        idx_dir = read_guts(guts, "idx_dir")
        for dump in glob.glob(os.path.join(
                idx_dir, "@fts", "dumps", "*.dump.txt")):
            all_tasks.append(CollectFileTask(f"Collecting file {dump}", dump))

    addr = zip_node.split("@")[-1]
    if addr == "127.0.0.1" or addr == "::1":
        zip_node = '@'.join(zip_node.split(
            "@")[:-1] + [find_primary_addr(ipv6, addr)])

    if options.verbosity:
        log("Python version: %s" % sys.version)

    runner.run_tasks(*all_tasks)
    log_task = runner.literal_task("cbcollect_info log", log_stream.getvalue(),
                                   log_file="cbcollect_info.log",
                                   no_header=True)
    runner.run_tasks(log_task)
    runner.close()

    cleanup_old_prometheus_stats(guts)

    if upload_url and options.redact_level != "none":
        do_upload_and_exit(redact_zip_file, upload_url, options.upload_proxy,
                           tmp_dir=options.tmp_dir)
    elif upload_url:
        do_upload_and_exit(zip_filename, upload_url, options.upload_proxy,
                           tmp_dir=options.tmp_dir)


def find_primary_addr(ipv6, default=None):
    Family = socket.AF_INET6 if ipv6 else socket.AF_INET
    DnsAddr = "2001:4860:4860::8844" if ipv6 else "8.8.8.8"
    s = socket.socket(Family, socket.SOCK_DGRAM)
    try:
        s.connect((DnsAddr, 56))
        if ipv6:
            addr, port, _, _ = s.getsockname()
        else:
            addr, port = s.getsockname()

        return addr
    except socket.error:
        return default
    finally:
        s.close()


def exec_name(name):
    if sys.platform == 'win32':
        name += ".exe"
    return name


if __name__ == '__main__':
    main()
